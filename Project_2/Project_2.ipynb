{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2: Regressão Logística e Redes Neurais\n",
    "\n",
    "O projeto consiste em explorar técnicas de classificação utilizando o _dataset_ [CINIC-10](https://github.com/BayesWatch/cinic-10), que consiste em um conjunto de 100000 imagens anotadas em 10 classes, que são:\n",
    "\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck\n",
    "\n",
    "Para resolver o problema, serão implementadas soluções que utilizam regressão logística multinomial ou rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed, shuffle, sample\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _dataset_ a ser utilizado já está separado em conjuntos de treinamento, validação e teste, em arquivos **.npz**. Inicialmente, consideraremos apenas os conjuntos de treinamento e validação. Podemos economizar memória lendo os valores alvo como _int8_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('Dataset/train.npz')\n",
    "valid = np.load('Dataset/val.npz')\n",
    "X, X_v  = train['xs'], valid['xs']\n",
    "Y, Y_v = train['ys'].astype('int8') , valid['ys'].astype('int8')\n",
    "\n",
    "# Constantes de classe\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados em memória, podemos **normalizá-los** de diversas maneiras. As estatísticas utilizadas para normalização são as mesmas para todos os conjuntos. Esse processo pode consumir bastante memória então, para economia, sempre que possível os tipos serão alterados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(data, choice=1):\n",
    "    means,stds,mins,ranges = [],[],[],[]\n",
    "    \n",
    "    # Stats for normalization\n",
    "    if choice == 1 or choice == 3:\n",
    "        mins = np.apply_along_axis(np.amin, 0, data).astype('int16')\n",
    "        maxs = np.apply_along_axis(np.amax, 0, data).astype('int16')\n",
    "        ranges = maxs - mins\n",
    "    if choice == 2 or choice == 3:\n",
    "        means = np.apply_along_axis(np.mean, 0, data).astype('float16')\n",
    "    if choice == 2:\n",
    "        stds = np.apply_along_axis(np.std, 0, data).astype('float16')\n",
    "    \n",
    "    return {'mean':means, 'std':stds, 'mins':mins, 'range':ranges}\n",
    "    \n",
    "def normalize_data(data, stats, choice=1):\n",
    "    ''' Returns the normalized dataset.\n",
    "    \n",
    "        Parameters:\n",
    "            data (array) : numpy array with the dataset.\n",
    "            stats (array): numpy array with stats given by \"get_stats\". 0: means. 1: stds. 2:mins. 3:ranges\n",
    "            choice (int) : integer indicating the transformation to be used.\n",
    "\n",
    "        Returns:\n",
    "            data (array list):  transformed data (original data is lost).\n",
    "    '''\n",
    "\n",
    "    #### Transforming the dataset ####\n",
    "    # Min-max normalization\n",
    "    if choice == 1:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mins'])/stats['range'], 1, data).astype('float32')\n",
    "            \n",
    "    # Standardization\n",
    "    elif choice == 2:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mean'])/stats['std'], 1, data).astype('float32')\n",
    "            \n",
    "    # Mean normalization\n",
    "    elif choice == 3:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mean'])/stats['range'], 1, data).astype('float32')\n",
    "\n",
    "    return data\n",
    "\n",
    "def out_layers(array):\n",
    "    '''Converts values from a list to one hot enconded output layers\n",
    "\n",
    "        Parameters:\n",
    "            array (list of ints): Contains the integer ID for each class\n",
    "    \n",
    "        Returns:\n",
    "            np.ndarray : 2D array where each colum is a output layer\n",
    "    '''\n",
    "    lower = min(array)\n",
    "    upper = max(array)\n",
    "    lines = upper-lower+1\n",
    "    new_arr = np.zeros((lines,len(array)), dtype=np.int8)\n",
    "\n",
    "    for j,i in enumerate(map(lambda x : x-lower, array)):\n",
    "       new_arr[i,j] = 1\n",
    "\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados normalizados, podemos começar a trabalhar com eles. Primeiramente, foi implementada uma **Regressão Logística Multinomial** para tentar solucionar o problema, utilizando a função _softmax_ para múltiplas classes. \n",
    "\n",
    "Foi utilizado **Batch Gradient Descent**, sem _early stopping_ e salvando o melhor conjunto de coeficientes.\n",
    "\n",
    "Para melhor visualização, os custos em cada conjunto por época foram guardados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def softmax(x):\n",
    "    x -= np.max(x, axis=1, keepdims=True)          # Numeric Stability\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def prob(X, T):\n",
    "    return softmax(X.dot(T))\n",
    "\n",
    "def predict(X, T):\n",
    "    y_scores = X.dot(T)\n",
    "    return np.argmax(y_scores, axis=1)\n",
    "\n",
    "# Cost function\n",
    "def cost(Y, Y_probs):\n",
    "    correct_probs = Y_probs[np.arange(Y.size), Y]\n",
    "    return (-1 * log(correct_probs)).mean()\n",
    "\n",
    "def cost_derivative(X, Y, Y_probs):\n",
    "    Y_probs[np.arange(Y.size),Y] -= 1\n",
    "    Y_probs /= Y.size\n",
    "    return X.T.dot(Y_probs)\n",
    "\n",
    "def log(x, bound=1e-16):\n",
    "    return np.log(np.maximum(x,bound))\n",
    "\n",
    "# Gradient Descent\n",
    "def gd_step(X, Y, T, Y_prob, alpha):\n",
    "    return T - alpha * cost_derivative(X, Y, Y_prob)\n",
    "\n",
    "def gradient_descent(X, Y, X_v, Y_v, T, alpha=0.01, e_lim=300):\n",
    "    \n",
    "    # First losses and scores\n",
    "    Y_prob = prob(X, T)\n",
    "    Y_v_prob = prob(X_v, T)\n",
    "    loss = cost(Y, Y_prob)\n",
    "    best_loss = cost(Y_v, Y_v_prob)\n",
    "\n",
    "    # History\n",
    "    best_T = T.copy()\n",
    "    history = {'cost': [loss], 'v_cost': [best_loss]}\n",
    "    \n",
    "    # Descent\n",
    "    for i in range(e_lim):\n",
    "        \n",
    "        # New theta\n",
    "        T = gd_step(X, Y, T, Y_prob, alpha)\n",
    "        \n",
    "        # New scores and losses\n",
    "        Y_prob = prob(X, T)\n",
    "        loss = cost(Y, Y_prob)\n",
    "        \n",
    "        # Validation\n",
    "        Y_v_prob = prob(X_v, T)\n",
    "        v_loss = cost(Y_v, Y_v_prob)\n",
    "        \n",
    "        # Updating best loss\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            best_T = T.copy()\n",
    "        \n",
    "        # History\n",
    "        history['cost'].append(loss)\n",
    "        history['v_cost'].append(v_loss)\n",
    "        print(f\"Epoch {i+1:04d}/{e_lim:04d}\", f\"loss: {loss:.4f} | val loss: {v_loss:.4f}\")\n",
    "    \n",
    "    print()\n",
    "    return best_T, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a regressão pronta, basta iniciar os coeficientes para o treinamento. Para isso, foi utilizada a _Xavier Initialization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coefs(features, dim2, rand_seed=None):\n",
    "    rand = np.random.RandomState(seed=rand_seed)\n",
    "    return np.sqrt(2/(features + 1)) * rand.randn(features, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(X, X_v, Y, Y_v, alpha, n_epochs, n_classes):\n",
    "    '''\n",
    "        Runs multinomial logistic regression.\n",
    "        Uses Xavier Random Initialization of coefficients.\n",
    "        Gradient descent: softmax function.\n",
    "    '''\n",
    "    \n",
    "    T = init_coefs(X.shape[1], n_classes, 57).astype('float32')\n",
    "\n",
    "    print(\"Regression:\")\n",
    "    T, hist = gradient_descent(X, Y, X_v, Y_v, T, alpha, n_epochs)\n",
    "    return T, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a regressão, pode-se verificar as curvas de aprendizado de treinamento e de validação por meio dos valores salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_with_history(history):\n",
    "    '''\n",
    "        Plots learning curves from history (dictionary of lists)\n",
    "    '''\n",
    "    keys = sorted(history.keys())\n",
    "    for k in keys:\n",
    "        plt.plot(history[k])\n",
    "        \n",
    "    plt.legend(keys, loc='upper left')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar o desempenho do modelo, diversas métricas podem ser utilizadas, como:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "Porém, para múltiplas classes, apenas _accuracy_ possui definição escalável. As outras métricas estão definidas **por classe**, e a média pode ser utilizada como métrica única se desejado. Um modo fácil de calcular essas métricas é com a **matriz de confusão** da solução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the results.\n",
    "def confusion_matrix(Y, Y_pred, classes):\n",
    "    conf = np.zeros((classes,classes)).astype('int32')\n",
    "    for i in range(Y.size):\n",
    "        conf[Y[i], Y_pred[i]] += 1\n",
    "    return conf\n",
    "\n",
    "# Accuracy from confusion matrix. True/total\n",
    "def accuracy(confusion):\n",
    "    true_amount = confusion.trace()\n",
    "    total = confusion.sum()\n",
    "    return true_amount/total\n",
    "    \n",
    "# Precision per class from confusion matrix.\n",
    "def precision(confusion):\n",
    "    diag = np.diagonal(confusion)\n",
    "    return diag/confusion.sum(0)\n",
    "\n",
    "# Recall per class from confusion matrix.\n",
    "def recall(confusion):\n",
    "    diag = np.diagonal(confusion)\n",
    "    return diag/confusion.sum(1)\n",
    "\n",
    "# F1 Score per class from precision and recall\n",
    "def f1_score(prec, rec, bound=1):\n",
    "    return 2*prec*rec/(prec+rec+bound)\n",
    "    \n",
    "\n",
    "# Function to calculate metrics for evaluation\n",
    "def get_metrics(target, predictions, classes):\n",
    "    conf = confusion_matrix(target, predictions, classes)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy(conf)\n",
    "    prec= precision(conf)\n",
    "    rec = recall(conf)\n",
    "    f1 = f1_score(prec, rec)\n",
    "    avg_acc = (prec + rec)/2\n",
    "    \n",
    "    return {'accuracy':acc, 'norm_acc':avg_acc, 'precision':prec, 'recall':rec, 'f1':f1}, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz de confusão em si também é interessante de visualizar, por conter muitas informações pertinentes ao desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion, classes, model='Neural Network'):\n",
    "    '''\n",
    "        Plots an already created confusion matrix for a generic amount of classes.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    #Bounding box\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['right'].set_color('black')\n",
    "\n",
    "    plt.title('Confusion Matrix for ' + model)\n",
    "\n",
    "    #Ticks\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thr = confusion.max()/2\n",
    "    for i, j in it.product(range(confusion.shape[0]), range(confusion.shape[1])):\n",
    "        plt.text(j, i, confusion[i, j],\n",
    "            horizontalalignment='center',\n",
    "            color='white' if confusion[i, j] > thr else 'black')\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.imshow(confusion, interpolation='nearest', cmap='Blues')\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic(X, Y, T, classes):\n",
    "    '''\n",
    "        Tests logistic regression with implemented metrics.\n",
    "        Accuracy, normalized accuracy, precision, recall, f1-score.\n",
    "    '''\n",
    "    \n",
    "    pred = predict(X, T)\n",
    "    met, conf = get_metrics(Y, pred, len(classes)) \n",
    "    plot_confusion_matrix(conf, classes, model = 'Multinomial Logistic Regression')\n",
    "    \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(f'Accuracy: {met[\"accuracy\"]:.4f}')\n",
    "    print(f'Normalized Accuracy: {met[\"norm_acc\"].mean():.4f}')\n",
    "    print(f'Precision per class: {met[\"precision\"]} (avg. precision: {met[\"precision\"].mean():.4f})')\n",
    "    print(f'Recall per class: {met[\"recall\"]} (avg. recall: {met[\"recall\"].mean():.4f})')\n",
    "    print(f'F1-Score per class: {met[\"f1\"]} (avg. f1-score: {met[\"f1\"].mean():.4f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfim, adicionando o _bias_ , coluna adicional no início do conjunto de exemplos, para representar o termo _intercept_ , podemos treinar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após muitos testes, parâmetros bons para a regressão definidos foram _learning rate_ de 0.01 e limite de 300 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "choice = 2\n",
    "stats = get_stats(X, choice)\n",
    "X = normalize_data(X, stats, choice)\n",
    "print(\"Training Data Normalized!\")\n",
    "X_v = normalize_data(X_v, stats, choice)\n",
    "print(\"Validation Data Normalized!\")\n",
    "\n",
    "Xb = np.insert(X, 0, 1, axis=1)\n",
    "X_vb = np.insert(X_v, 0, 1, axis=1)\n",
    "print(\"Bias Added\")\n",
    "\n",
    "#### MULTINOMIAL LOGISTIC REGRESSION ####\n",
    "T, history = run_logistic(Xb, X_vb, Y, Y_v, 0.01, 300, len(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_with_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "print(\"Training Metrics:\")\n",
    "test_logistic(Xb, Y, T, classes)\n",
    "    \n",
    "print()\n",
    "print(\"Validation Metrics:\")\n",
    "test_logistic(X_vb, Y_v, T, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
