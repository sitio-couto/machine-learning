{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2: Regressão Logística e Redes Neurais\n",
    "\n",
    "O projeto consiste em explorar técnicas de classificação utilizando o _dataset_ [CINIC-10](https://github.com/BayesWatch/cinic-10), que consiste em um conjunto de 100000 imagens anotadas em 10 classes, que são:\n",
    "\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck\n",
    "\n",
    "Para resolver o problema, serão implementadas soluções que utilizam regressão logística multinomial ou rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed, shuffle, sample\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _dataset_ a ser utilizado já está separado em conjuntos de treinamento, validação e teste, em arquivos **.npz**. Inicialmente, consideraremos apenas os conjuntos de treinamento e validação. Podemos economizar memória lendo os valores alvo como _int8_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('Dataset/train.npz')\n",
    "valid = np.load('Dataset/val.npz')\n",
    "X, X_v  = train['xs'], valid['xs']\n",
    "Y, Y_v = train['ys'].astype('int8') , valid['ys'].astype('int8')\n",
    "\n",
    "# Constantes de classe\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados em memória, podemos **normalizá-los** de diversas maneiras. As estatísticas utilizadas para normalização são as mesmas para todos os conjuntos. Esse processo pode consumir bastante memória então, para economia, sempre que possível os tipos serão alterados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(data, choice=1):\n",
    "    means,stds,mins,ranges = [],[],[],[]\n",
    "    \n",
    "    # Stats for normalization\n",
    "    if choice == 1 or choice == 3:\n",
    "        mins = np.apply_along_axis(np.amin, 0, data).astype('int16')\n",
    "        maxs = np.apply_along_axis(np.amax, 0, data).astype('int16')\n",
    "        ranges = maxs - mins\n",
    "    if choice == 2 or choice == 3:\n",
    "        means = np.apply_along_axis(np.mean, 0, data).astype('float16')\n",
    "    if choice == 2:\n",
    "        stds = np.apply_along_axis(np.std, 0, data).astype('float16')\n",
    "    \n",
    "    return {'mean':means, 'std':stds, 'mins':mins, 'range':ranges}\n",
    "    \n",
    "def normalize_data(data, stats, choice=1):\n",
    "    ''' Returns the normalized dataset.\n",
    "    \n",
    "        Parameters:\n",
    "            data (array) : numpy array with the dataset.\n",
    "            stats (array): numpy array with stats given by \"get_stats\". 0: means. 1: stds. 2:mins. 3:ranges\n",
    "            choice (int) : integer indicating the transformation to be used.\n",
    "\n",
    "        Returns:\n",
    "            data (array list):  transformed data (original data is lost).\n",
    "    '''\n",
    "\n",
    "    #### Transforming the dataset ####\n",
    "    # Min-max normalization\n",
    "    if choice == 1:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mins'])/stats['range'], 1, data).astype('float32')\n",
    "            \n",
    "    # Standardization\n",
    "    elif choice == 2:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mean'])/stats['std'], 1, data).astype('float32')\n",
    "            \n",
    "    # Mean normalization\n",
    "    elif choice == 3:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mean'])/stats['range'], 1, data).astype('float32')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados normalizados, podemos começar a trabalhar com eles. Primeiramente, foi implementada uma **Regressão Logística Multinomial** para tentar solucionar o problema, utilizando a função _softmax_ para múltiplas classes. \n",
    "\n",
    "Foi utilizado **Batch Gradient Descent**, sem _early stopping_ e salvando o melhor conjunto de coeficientes.\n",
    "\n",
    "Para melhor visualização, os custos em cada conjunto por época foram guardados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def softmax(x):\n",
    "    x -= np.max(x, axis=1, keepdims=True)          # Numeric Stability\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def prob(X, T):\n",
    "    return softmax(X.dot(T))\n",
    "\n",
    "def predict(X, T):\n",
    "    y_scores = X.dot(T)\n",
    "    return np.argmax(y_scores, axis=1)\n",
    "\n",
    "# Cost function\n",
    "def cost(Y, Y_probs):\n",
    "    correct_probs = Y_probs[np.arange(Y.size), Y]\n",
    "    return (-1 * log(correct_probs)).mean()\n",
    "\n",
    "def cost_derivative(X, Y, Y_probs):\n",
    "    Y_probs[np.arange(Y.size),Y] -= 1\n",
    "    Y_probs /= Y.size\n",
    "    return X.T.dot(Y_probs)\n",
    "\n",
    "def log(x, bound=1e-16):\n",
    "    return np.log(np.maximum(x,bound))\n",
    "\n",
    "# Gradient Descent\n",
    "def gd_step(X, Y, T, Y_prob, alpha):\n",
    "    return T - alpha * cost_derivative(X, Y, Y_prob)\n",
    "\n",
    "def gradient_descent(X, Y, X_v, Y_v, T, alpha=0.01, e_lim=300):\n",
    "    \n",
    "    # First losses and scores\n",
    "    Y_prob = prob(X, T)\n",
    "    Y_v_prob = prob(X_v, T)\n",
    "    loss = cost(Y, Y_prob)\n",
    "    best_loss = cost(Y_v, Y_v_prob)\n",
    "\n",
    "    # History\n",
    "    best_T = T.copy()\n",
    "    history = {'cost': [loss], 'v_cost': [best_loss]}\n",
    "    \n",
    "    # Descent\n",
    "    for i in range(e_lim):\n",
    "        \n",
    "        # New theta\n",
    "        T = gd_step(X, Y, T, Y_prob, alpha)\n",
    "        \n",
    "        # New scores and losses\n",
    "        Y_prob = prob(X, T)\n",
    "        loss = cost(Y, Y_prob)\n",
    "        \n",
    "        # Validation\n",
    "        Y_v_prob = prob(X_v, T)\n",
    "        v_loss = cost(Y_v, Y_v_prob)\n",
    "        \n",
    "        # Updating best loss\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            best_T = T.copy()\n",
    "        \n",
    "        # History\n",
    "        history['cost'].append(loss)\n",
    "        history['v_cost'].append(v_loss)\n",
    "        print(f\"Epoch {i+1:04d}/{e_lim:04d}\", f\"loss: {loss:.4f} | val loss: {v_loss:.4f}\")\n",
    "    \n",
    "    print()\n",
    "    return best_T, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a regressão pronta, basta iniciar os coeficientes para o treinamento. Para isso, foi utilizada a _Xavier Initialization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coefs(features, dim2, rand_seed=None):\n",
    "    rand = np.random.RandomState(seed=rand_seed)\n",
    "    return np.sqrt(2/(features + 1)) * rand.randn(features, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(X, X_v, Y, Y_v, alpha, n_epochs, n_classes):\n",
    "    '''\n",
    "        Runs multinomial logistic regression.\n",
    "        Uses Xavier Random Initialization of coefficients.\n",
    "        Gradient descent: softmax function.\n",
    "    '''\n",
    "    \n",
    "    T = init_coefs(X.shape[1], n_classes, 57).astype('float32')\n",
    "\n",
    "    print(\"Regression:\")\n",
    "    T, hist = gradient_descent(X, Y, X_v, Y_v, T, alpha, n_epochs)\n",
    "    return T, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando o _bias_ , coluna adicional no início do conjunto de exemplos, para representar o termo _intercept_ , podemos treinar o modelo.\n",
    "Após muitos testes, parâmetros bons para a regressão definidos foram _learning rate_ de 0.01 e limite de 300 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Normalized!\n",
      "Validation Data Normalized!\n",
      "Bias Added\n",
      "Regression:\n",
      "Epoch 0001/0300 loss: 2.8951 | val loss: 2.9115\n",
      "Epoch 0002/0300 loss: 2.6790 | val loss: 2.6988\n",
      "Epoch 0003/0300 loss: 2.5843 | val loss: 2.6048\n",
      "Epoch 0004/0300 loss: 2.5258 | val loss: 2.5455\n",
      "Epoch 0005/0300 loss: 2.4824 | val loss: 2.5013\n",
      "Epoch 0006/0300 loss: 2.4477 | val loss: 2.4656\n",
      "Epoch 0007/0300 loss: 2.4186 | val loss: 2.4355\n",
      "Epoch 0008/0300 loss: 2.3936 | val loss: 2.4096\n",
      "Epoch 0009/0300 loss: 2.3717 | val loss: 2.3869\n",
      "Epoch 0010/0300 loss: 2.3523 | val loss: 2.3667\n",
      "Epoch 0011/0300 loss: 2.3349 | val loss: 2.3486\n",
      "Epoch 0012/0300 loss: 2.3192 | val loss: 2.3323\n",
      "Epoch 0013/0300 loss: 2.3050 | val loss: 2.3175\n",
      "Epoch 0014/0300 loss: 2.2920 | val loss: 2.3040\n",
      "Epoch 0015/0300 loss: 2.2801 | val loss: 2.2916\n",
      "Epoch 0016/0300 loss: 2.2691 | val loss: 2.2802\n",
      "Epoch 0017/0300 loss: 2.2590 | val loss: 2.2696\n",
      "Epoch 0018/0300 loss: 2.2496 | val loss: 2.2599\n",
      "Epoch 0019/0300 loss: 2.2408 | val loss: 2.2508\n",
      "Epoch 0020/0300 loss: 2.2326 | val loss: 2.2423\n",
      "Epoch 0021/0300 loss: 2.2249 | val loss: 2.2344\n",
      "Epoch 0022/0300 loss: 2.2177 | val loss: 2.2269\n",
      "Epoch 0023/0300 loss: 2.2109 | val loss: 2.2199\n",
      "Epoch 0024/0300 loss: 2.2045 | val loss: 2.2133\n",
      "Epoch 0025/0300 loss: 2.1984 | val loss: 2.2071\n",
      "Epoch 0026/0300 loss: 2.1927 | val loss: 2.2012\n",
      "Epoch 0027/0300 loss: 2.1872 | val loss: 2.1956\n",
      "Epoch 0028/0300 loss: 2.1820 | val loss: 2.1903\n",
      "Epoch 0029/0300 loss: 2.1771 | val loss: 2.1852\n",
      "Epoch 0030/0300 loss: 2.1723 | val loss: 2.1804\n",
      "Epoch 0031/0300 loss: 2.1678 | val loss: 2.1758\n",
      "Epoch 0032/0300 loss: 2.1635 | val loss: 2.1713\n",
      "Epoch 0033/0300 loss: 2.1593 | val loss: 2.1671\n",
      "Epoch 0034/0300 loss: 2.1553 | val loss: 2.1631\n",
      "Epoch 0035/0300 loss: 2.1515 | val loss: 2.1592\n",
      "Epoch 0036/0300 loss: 2.1478 | val loss: 2.1554\n",
      "Epoch 0037/0300 loss: 2.1442 | val loss: 2.1518\n",
      "Epoch 0038/0300 loss: 2.1407 | val loss: 2.1483\n",
      "Epoch 0039/0300 loss: 2.1374 | val loss: 2.1450\n",
      "Epoch 0040/0300 loss: 2.1342 | val loss: 2.1417\n",
      "Epoch 0041/0300 loss: 2.1311 | val loss: 2.1386\n",
      "Epoch 0042/0300 loss: 2.1281 | val loss: 2.1356\n",
      "Epoch 0043/0300 loss: 2.1252 | val loss: 2.1326\n",
      "Epoch 0044/0300 loss: 2.1223 | val loss: 2.1298\n",
      "Epoch 0045/0300 loss: 2.1196 | val loss: 2.1271\n",
      "Epoch 0046/0300 loss: 2.1169 | val loss: 2.1244\n",
      "Epoch 0047/0300 loss: 2.1143 | val loss: 2.1218\n",
      "Epoch 0048/0300 loss: 2.1118 | val loss: 2.1193\n",
      "Epoch 0049/0300 loss: 2.1094 | val loss: 2.1168\n",
      "Epoch 0050/0300 loss: 2.1070 | val loss: 2.1144\n",
      "Epoch 0051/0300 loss: 2.1046 | val loss: 2.1121\n",
      "Epoch 0052/0300 loss: 2.1024 | val loss: 2.1098\n",
      "Epoch 0053/0300 loss: 2.1002 | val loss: 2.1076\n",
      "Epoch 0054/0300 loss: 2.0980 | val loss: 2.1055\n",
      "Epoch 0055/0300 loss: 2.0959 | val loss: 2.1034\n",
      "Epoch 0056/0300 loss: 2.0938 | val loss: 2.1013\n",
      "Epoch 0057/0300 loss: 2.0918 | val loss: 2.0993\n",
      "Epoch 0058/0300 loss: 2.0898 | val loss: 2.0974\n",
      "Epoch 0059/0300 loss: 2.0879 | val loss: 2.0955\n",
      "Epoch 0060/0300 loss: 2.0860 | val loss: 2.0936\n",
      "Epoch 0061/0300 loss: 2.0842 | val loss: 2.0918\n",
      "Epoch 0062/0300 loss: 2.0824 | val loss: 2.0900\n",
      "Epoch 0063/0300 loss: 2.0806 | val loss: 2.0883\n",
      "Epoch 0064/0300 loss: 2.0789 | val loss: 2.0866\n",
      "Epoch 0065/0300 loss: 2.0772 | val loss: 2.0849\n",
      "Epoch 0066/0300 loss: 2.0755 | val loss: 2.0832\n",
      "Epoch 0067/0300 loss: 2.0739 | val loss: 2.0816\n",
      "Epoch 0068/0300 loss: 2.0723 | val loss: 2.0801\n",
      "Epoch 0069/0300 loss: 2.0707 | val loss: 2.0785\n",
      "Epoch 0070/0300 loss: 2.0692 | val loss: 2.0770\n",
      "Epoch 0071/0300 loss: 2.0676 | val loss: 2.0755\n",
      "Epoch 0072/0300 loss: 2.0662 | val loss: 2.0741\n",
      "Epoch 0073/0300 loss: 2.0647 | val loss: 2.0726\n",
      "Epoch 0074/0300 loss: 2.0633 | val loss: 2.0712\n",
      "Epoch 0075/0300 loss: 2.0619 | val loss: 2.0698\n",
      "Epoch 0076/0300 loss: 2.0605 | val loss: 2.0685\n",
      "Epoch 0077/0300 loss: 2.0591 | val loss: 2.0671\n",
      "Epoch 0078/0300 loss: 2.0578 | val loss: 2.0658\n",
      "Epoch 0079/0300 loss: 2.0564 | val loss: 2.0645\n",
      "Epoch 0080/0300 loss: 2.0551 | val loss: 2.0633\n",
      "Epoch 0081/0300 loss: 2.0539 | val loss: 2.0620\n",
      "Epoch 0082/0300 loss: 2.0526 | val loss: 2.0608\n",
      "Epoch 0083/0300 loss: 2.0514 | val loss: 2.0596\n",
      "Epoch 0084/0300 loss: 2.0502 | val loss: 2.0584\n",
      "Epoch 0085/0300 loss: 2.0490 | val loss: 2.0573\n",
      "Epoch 0086/0300 loss: 2.0478 | val loss: 2.0561\n",
      "Epoch 0087/0300 loss: 2.0466 | val loss: 2.0550\n",
      "Epoch 0088/0300 loss: 2.0455 | val loss: 2.0539\n",
      "Epoch 0089/0300 loss: 2.0443 | val loss: 2.0528\n",
      "Epoch 0090/0300 loss: 2.0432 | val loss: 2.0517\n",
      "Epoch 0091/0300 loss: 2.0421 | val loss: 2.0506\n",
      "Epoch 0092/0300 loss: 2.0410 | val loss: 2.0496\n",
      "Epoch 0093/0300 loss: 2.0400 | val loss: 2.0486\n",
      "Epoch 0094/0300 loss: 2.0389 | val loss: 2.0475\n",
      "Epoch 0095/0300 loss: 2.0379 | val loss: 2.0465\n",
      "Epoch 0096/0300 loss: 2.0369 | val loss: 2.0455\n",
      "Epoch 0097/0300 loss: 2.0358 | val loss: 2.0446\n",
      "Epoch 0098/0300 loss: 2.0348 | val loss: 2.0436\n",
      "Epoch 0099/0300 loss: 2.0339 | val loss: 2.0427\n",
      "Epoch 0100/0300 loss: 2.0329 | val loss: 2.0417\n",
      "Epoch 0101/0300 loss: 2.0319 | val loss: 2.0408\n",
      "Epoch 0102/0300 loss: 2.0310 | val loss: 2.0399\n",
      "Epoch 0103/0300 loss: 2.0300 | val loss: 2.0390\n",
      "Epoch 0104/0300 loss: 2.0291 | val loss: 2.0381\n",
      "Epoch 0105/0300 loss: 2.0282 | val loss: 2.0372\n",
      "Epoch 0106/0300 loss: 2.0273 | val loss: 2.0364\n",
      "Epoch 0107/0300 loss: 2.0264 | val loss: 2.0355\n",
      "Epoch 0108/0300 loss: 2.0255 | val loss: 2.0347\n",
      "Epoch 0109/0300 loss: 2.0247 | val loss: 2.0338\n",
      "Epoch 0110/0300 loss: 2.0238 | val loss: 2.0330\n",
      "Epoch 0111/0300 loss: 2.0229 | val loss: 2.0322\n",
      "Epoch 0112/0300 loss: 2.0221 | val loss: 2.0314\n",
      "Epoch 0113/0300 loss: 2.0213 | val loss: 2.0306\n",
      "Epoch 0114/0300 loss: 2.0205 | val loss: 2.0298\n",
      "Epoch 0115/0300 loss: 2.0196 | val loss: 2.0290\n",
      "Epoch 0116/0300 loss: 2.0188 | val loss: 2.0283\n",
      "Epoch 0117/0300 loss: 2.0180 | val loss: 2.0275\n",
      "Epoch 0118/0300 loss: 2.0173 | val loss: 2.0268\n",
      "Epoch 0119/0300 loss: 2.0165 | val loss: 2.0260\n",
      "Epoch 0120/0300 loss: 2.0157 | val loss: 2.0253\n",
      "Epoch 0121/0300 loss: 2.0149 | val loss: 2.0246\n",
      "Epoch 0122/0300 loss: 2.0142 | val loss: 2.0239\n",
      "Epoch 0123/0300 loss: 2.0134 | val loss: 2.0232\n",
      "Epoch 0124/0300 loss: 2.0127 | val loss: 2.0225\n",
      "Epoch 0125/0300 loss: 2.0120 | val loss: 2.0218\n",
      "Epoch 0126/0300 loss: 2.0112 | val loss: 2.0211\n",
      "Epoch 0127/0300 loss: 2.0105 | val loss: 2.0204\n",
      "Epoch 0128/0300 loss: 2.0098 | val loss: 2.0197\n",
      "Epoch 0129/0300 loss: 2.0091 | val loss: 2.0191\n",
      "Epoch 0130/0300 loss: 2.0084 | val loss: 2.0184\n",
      "Epoch 0131/0300 loss: 2.0077 | val loss: 2.0178\n",
      "Epoch 0132/0300 loss: 2.0070 | val loss: 2.0171\n",
      "Epoch 0133/0300 loss: 2.0064 | val loss: 2.0165\n",
      "Epoch 0134/0300 loss: 2.0057 | val loss: 2.0159\n",
      "Epoch 0135/0300 loss: 2.0050 | val loss: 2.0152\n",
      "Epoch 0136/0300 loss: 2.0044 | val loss: 2.0146\n",
      "Epoch 0137/0300 loss: 2.0037 | val loss: 2.0140\n",
      "Epoch 0138/0300 loss: 2.0031 | val loss: 2.0134\n",
      "Epoch 0139/0300 loss: 2.0024 | val loss: 2.0128\n",
      "Epoch 0140/0300 loss: 2.0018 | val loss: 2.0122\n",
      "Epoch 0141/0300 loss: 2.0012 | val loss: 2.0116\n",
      "Epoch 0142/0300 loss: 2.0006 | val loss: 2.0111\n",
      "Epoch 0143/0300 loss: 1.9999 | val loss: 2.0105\n",
      "Epoch 0144/0300 loss: 1.9993 | val loss: 2.0099\n",
      "Epoch 0145/0300 loss: 1.9987 | val loss: 2.0093\n",
      "Epoch 0146/0300 loss: 1.9981 | val loss: 2.0088\n",
      "Epoch 0147/0300 loss: 1.9975 | val loss: 2.0082\n",
      "Epoch 0148/0300 loss: 1.9969 | val loss: 2.0077\n",
      "Epoch 0149/0300 loss: 1.9963 | val loss: 2.0071\n",
      "Epoch 0150/0300 loss: 1.9958 | val loss: 2.0066\n",
      "Epoch 0151/0300 loss: 1.9952 | val loss: 2.0061\n",
      "Epoch 0152/0300 loss: 1.9946 | val loss: 2.0055\n",
      "Epoch 0153/0300 loss: 1.9940 | val loss: 2.0050\n",
      "Epoch 0154/0300 loss: 1.9935 | val loss: 2.0045\n",
      "Epoch 0155/0300 loss: 1.9929 | val loss: 2.0040\n",
      "Epoch 0156/0300 loss: 1.9924 | val loss: 2.0034\n",
      "Epoch 0157/0300 loss: 1.9918 | val loss: 2.0029\n",
      "Epoch 0158/0300 loss: 1.9913 | val loss: 2.0024\n",
      "Epoch 0159/0300 loss: 1.9907 | val loss: 2.0019\n",
      "Epoch 0160/0300 loss: 1.9902 | val loss: 2.0014\n",
      "Epoch 0161/0300 loss: 1.9897 | val loss: 2.0009\n",
      "Epoch 0162/0300 loss: 1.9891 | val loss: 2.0005\n",
      "Epoch 0163/0300 loss: 1.9886 | val loss: 2.0000\n",
      "Epoch 0164/0300 loss: 1.9881 | val loss: 1.9995\n",
      "Epoch 0165/0300 loss: 1.9876 | val loss: 1.9990\n",
      "Epoch 0166/0300 loss: 1.9871 | val loss: 1.9986\n",
      "Epoch 0167/0300 loss: 1.9865 | val loss: 1.9981\n",
      "Epoch 0168/0300 loss: 1.9860 | val loss: 1.9976\n",
      "Epoch 0169/0300 loss: 1.9855 | val loss: 1.9972\n",
      "Epoch 0170/0300 loss: 1.9850 | val loss: 1.9967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0171/0300 loss: 1.9845 | val loss: 1.9963\n",
      "Epoch 0172/0300 loss: 1.9841 | val loss: 1.9958\n",
      "Epoch 0173/0300 loss: 1.9836 | val loss: 1.9954\n",
      "Epoch 0174/0300 loss: 1.9831 | val loss: 1.9949\n",
      "Epoch 0175/0300 loss: 1.9826 | val loss: 1.9945\n",
      "Epoch 0176/0300 loss: 1.9821 | val loss: 1.9940\n",
      "Epoch 0177/0300 loss: 1.9817 | val loss: 1.9936\n",
      "Epoch 0178/0300 loss: 1.9812 | val loss: 1.9932\n",
      "Epoch 0179/0300 loss: 1.9807 | val loss: 1.9928\n",
      "Epoch 0180/0300 loss: 1.9803 | val loss: 1.9923\n",
      "Epoch 0181/0300 loss: 1.9798 | val loss: 1.9919\n",
      "Epoch 0182/0300 loss: 1.9793 | val loss: 1.9915\n",
      "Epoch 0183/0300 loss: 1.9789 | val loss: 1.9911\n",
      "Epoch 0184/0300 loss: 1.9784 | val loss: 1.9907\n",
      "Epoch 0185/0300 loss: 1.9780 | val loss: 1.9903\n",
      "Epoch 0186/0300 loss: 1.9775 | val loss: 1.9899\n",
      "Epoch 0187/0300 loss: 1.9771 | val loss: 1.9895\n",
      "Epoch 0188/0300 loss: 1.9766 | val loss: 1.9891\n",
      "Epoch 0189/0300 loss: 1.9762 | val loss: 1.9887\n",
      "Epoch 0190/0300 loss: 1.9758 | val loss: 1.9883\n",
      "Epoch 0191/0300 loss: 1.9753 | val loss: 1.9879\n",
      "Epoch 0192/0300 loss: 1.9749 | val loss: 1.9875\n",
      "Epoch 0193/0300 loss: 1.9745 | val loss: 1.9871\n",
      "Epoch 0194/0300 loss: 1.9741 | val loss: 1.9867\n",
      "Epoch 0195/0300 loss: 1.9736 | val loss: 1.9864\n",
      "Epoch 0196/0300 loss: 1.9732 | val loss: 1.9860\n",
      "Epoch 0197/0300 loss: 1.9728 | val loss: 1.9856\n",
      "Epoch 0198/0300 loss: 1.9724 | val loss: 1.9852\n",
      "Epoch 0199/0300 loss: 1.9720 | val loss: 1.9849\n",
      "Epoch 0200/0300 loss: 1.9716 | val loss: 1.9845\n",
      "Epoch 0201/0300 loss: 1.9712 | val loss: 1.9841\n",
      "Epoch 0202/0300 loss: 1.9708 | val loss: 1.9838\n",
      "Epoch 0203/0300 loss: 1.9704 | val loss: 1.9834\n",
      "Epoch 0204/0300 loss: 1.9700 | val loss: 1.9831\n",
      "Epoch 0205/0300 loss: 1.9696 | val loss: 1.9827\n",
      "Epoch 0206/0300 loss: 1.9692 | val loss: 1.9823\n",
      "Epoch 0207/0300 loss: 1.9688 | val loss: 1.9820\n",
      "Epoch 0208/0300 loss: 1.9684 | val loss: 1.9816\n",
      "Epoch 0209/0300 loss: 1.9680 | val loss: 1.9813\n",
      "Epoch 0210/0300 loss: 1.9676 | val loss: 1.9810\n",
      "Epoch 0211/0300 loss: 1.9672 | val loss: 1.9806\n",
      "Epoch 0212/0300 loss: 1.9669 | val loss: 1.9803\n",
      "Epoch 0213/0300 loss: 1.9665 | val loss: 1.9799\n",
      "Epoch 0214/0300 loss: 1.9661 | val loss: 1.9796\n",
      "Epoch 0215/0300 loss: 1.9657 | val loss: 1.9793\n",
      "Epoch 0216/0300 loss: 1.9654 | val loss: 1.9789\n",
      "Epoch 0217/0300 loss: 1.9650 | val loss: 1.9786\n",
      "Epoch 0218/0300 loss: 1.9646 | val loss: 1.9783\n",
      "Epoch 0219/0300 loss: 1.9643 | val loss: 1.9780\n",
      "Epoch 0220/0300 loss: 1.9639 | val loss: 1.9776\n",
      "Epoch 0221/0300 loss: 1.9635 | val loss: 1.9773\n",
      "Epoch 0222/0300 loss: 1.9632 | val loss: 1.9770\n",
      "Epoch 0223/0300 loss: 1.9628 | val loss: 1.9767\n",
      "Epoch 0224/0300 loss: 1.9625 | val loss: 1.9764\n",
      "Epoch 0225/0300 loss: 1.9621 | val loss: 1.9761\n",
      "Epoch 0226/0300 loss: 1.9618 | val loss: 1.9758\n",
      "Epoch 0227/0300 loss: 1.9614 | val loss: 1.9754\n",
      "Epoch 0228/0300 loss: 1.9611 | val loss: 1.9751\n",
      "Epoch 0229/0300 loss: 1.9607 | val loss: 1.9748\n",
      "Epoch 0230/0300 loss: 1.9604 | val loss: 1.9745\n",
      "Epoch 0231/0300 loss: 1.9600 | val loss: 1.9742\n",
      "Epoch 0232/0300 loss: 1.9597 | val loss: 1.9739\n",
      "Epoch 0233/0300 loss: 1.9594 | val loss: 1.9736\n",
      "Epoch 0234/0300 loss: 1.9590 | val loss: 1.9733\n",
      "Epoch 0235/0300 loss: 1.9587 | val loss: 1.9730\n",
      "Epoch 0236/0300 loss: 1.9583 | val loss: 1.9728\n",
      "Epoch 0237/0300 loss: 1.9580 | val loss: 1.9725\n",
      "Epoch 0238/0300 loss: 1.9577 | val loss: 1.9722\n",
      "Epoch 0239/0300 loss: 1.9574 | val loss: 1.9719\n",
      "Epoch 0240/0300 loss: 1.9570 | val loss: 1.9716\n",
      "Epoch 0241/0300 loss: 1.9567 | val loss: 1.9713\n",
      "Epoch 0242/0300 loss: 1.9564 | val loss: 1.9710\n",
      "Epoch 0243/0300 loss: 1.9561 | val loss: 1.9707\n",
      "Epoch 0244/0300 loss: 1.9557 | val loss: 1.9705\n",
      "Epoch 0245/0300 loss: 1.9554 | val loss: 1.9702\n",
      "Epoch 0246/0300 loss: 1.9551 | val loss: 1.9699\n",
      "Epoch 0247/0300 loss: 1.9548 | val loss: 1.9696\n",
      "Epoch 0248/0300 loss: 1.9545 | val loss: 1.9694\n",
      "Epoch 0249/0300 loss: 1.9542 | val loss: 1.9691\n",
      "Epoch 0250/0300 loss: 1.9539 | val loss: 1.9688\n",
      "Epoch 0251/0300 loss: 1.9535 | val loss: 1.9686\n",
      "Epoch 0252/0300 loss: 1.9532 | val loss: 1.9683\n",
      "Epoch 0253/0300 loss: 1.9529 | val loss: 1.9680\n",
      "Epoch 0254/0300 loss: 1.9526 | val loss: 1.9678\n",
      "Epoch 0255/0300 loss: 1.9523 | val loss: 1.9675\n",
      "Epoch 0256/0300 loss: 1.9520 | val loss: 1.9672\n",
      "Epoch 0257/0300 loss: 1.9517 | val loss: 1.9670\n",
      "Epoch 0258/0300 loss: 1.9514 | val loss: 1.9667\n",
      "Epoch 0259/0300 loss: 1.9511 | val loss: 1.9665\n",
      "Epoch 0260/0300 loss: 1.9508 | val loss: 1.9662\n",
      "Epoch 0261/0300 loss: 1.9505 | val loss: 1.9659\n",
      "Epoch 0262/0300 loss: 1.9502 | val loss: 1.9657\n",
      "Epoch 0263/0300 loss: 1.9499 | val loss: 1.9654\n",
      "Epoch 0264/0300 loss: 1.9497 | val loss: 1.9652\n",
      "Epoch 0265/0300 loss: 1.9494 | val loss: 1.9649\n",
      "Epoch 0266/0300 loss: 1.9491 | val loss: 1.9647\n",
      "Epoch 0267/0300 loss: 1.9488 | val loss: 1.9644\n",
      "Epoch 0268/0300 loss: 1.9485 | val loss: 1.9642\n",
      "Epoch 0269/0300 loss: 1.9482 | val loss: 1.9640\n",
      "Epoch 0270/0300 loss: 1.9479 | val loss: 1.9637\n",
      "Epoch 0271/0300 loss: 1.9477 | val loss: 1.9635\n",
      "Epoch 0272/0300 loss: 1.9474 | val loss: 1.9632\n",
      "Epoch 0273/0300 loss: 1.9471 | val loss: 1.9630\n",
      "Epoch 0274/0300 loss: 1.9468 | val loss: 1.9627\n",
      "Epoch 0275/0300 loss: 1.9465 | val loss: 1.9625\n",
      "Epoch 0276/0300 loss: 1.9463 | val loss: 1.9623\n",
      "Epoch 0277/0300 loss: 1.9460 | val loss: 1.9620\n",
      "Epoch 0278/0300 loss: 1.9457 | val loss: 1.9618\n",
      "Epoch 0279/0300 loss: 1.9454 | val loss: 1.9616\n",
      "Epoch 0280/0300 loss: 1.9452 | val loss: 1.9613\n",
      "Epoch 0281/0300 loss: 1.9449 | val loss: 1.9611\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "choice = 2\n",
    "stats = get_stats(X, choice)\n",
    "X = normalize_data(X, stats, choice)\n",
    "print(\"Training Data Normalized!\")\n",
    "X_v = normalize_data(X_v, stats, choice)\n",
    "print(\"Validation Data Normalized!\")\n",
    "\n",
    "Xb = np.insert(X, 0, 1, axis=1)\n",
    "X_vb = np.insert(X_v, 0, 1, axis=1)\n",
    "print(\"Bias Added\")\n",
    "\n",
    "#### MULTINOMIAL LOGISTIC REGRESSION ####\n",
    "T, history = run_logistic(Xb, X_vb, Y, Y_v, 0.01, 300, len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a regressão, pode-se verificar as curvas de aprendizado de treinamento e de validação por meio dos valores salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_with_history(history):\n",
    "    '''\n",
    "        Plots learning curves from history (dictionary of lists)\n",
    "    '''\n",
    "    keys = sorted(history.keys())\n",
    "    for k in keys:\n",
    "        plt.plot(history[k])\n",
    "        \n",
    "    plt.legend(keys, loc='upper left')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "learning_with_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar o desempenho do modelo, diversas métricas podem ser utilizadas, como:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "Porém, para múltiplas classes, apenas _accuracy_ possui definição escalável. As outras métricas estão definidas **por classe**, e a média pode ser utilizada como métrica única se desejado. Um modo fácil de calcular essas métricas é com a **matriz de confusão** da solução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the results.\n",
    "def confusion_matrix(Y, Y_pred, classes):\n",
    "    conf = np.zeros((classes,classes)).astype('int32')\n",
    "    for i in range(Y.size):\n",
    "        conf[Y[i], Y_pred[i]] += 1\n",
    "    return conf\n",
    "\n",
    "# Accuracy from confusion matrix. True/total\n",
    "def accuracy(confusion):\n",
    "    true_amount = confusion.trace()\n",
    "    total = confusion.sum()\n",
    "    return true_amount/total\n",
    "    \n",
    "# Precision per class from confusion matrix.\n",
    "def precision(confusion):\n",
    "    diag = np.diagonal(confusion)\n",
    "    return diag/confusion.sum(0)\n",
    "\n",
    "# Recall per class from confusion matrix.\n",
    "def recall(confusion):\n",
    "    diag = np.diagonal(confusion)\n",
    "    return diag/confusion.sum(1)\n",
    "\n",
    "# F1 Score per class from precision and recall\n",
    "def f1_score(prec, rec, bound=1):\n",
    "    return 2*prec*rec/(prec+rec+bound)\n",
    "    \n",
    "\n",
    "# Function to calculate metrics for evaluation\n",
    "def get_metrics(target, predictions, classes):\n",
    "    conf = confusion_matrix(target, predictions, classes)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy(conf)\n",
    "    prec= precision(conf)\n",
    "    rec = recall(conf)\n",
    "    f1 = f1_score(prec, rec)\n",
    "    avg_acc = (prec + rec)/2\n",
    "    \n",
    "    return {'accuracy':acc, 'norm_acc':avg_acc, 'precision':prec, 'recall':rec, 'f1':f1}, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz de confusão em si também é interessante de visualizar, por conter muitas informações pertinentes ao desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion, classes, model='Neural Network'):\n",
    "    '''\n",
    "        Plots an already created confusion matrix for a generic amount of classes.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    #Bounding box\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['right'].set_color('black')\n",
    "\n",
    "    plt.title('Confusion Matrix for ' + model)\n",
    "\n",
    "    #Ticks\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thr = confusion.max()/2\n",
    "    for i, j in it.product(range(confusion.shape[0]), range(confusion.shape[1])):\n",
    "        plt.text(j, i, confusion[i, j],\n",
    "            horizontalalignment='center',\n",
    "            color='white' if confusion[i, j] > thr else 'black')\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.imshow(confusion, interpolation='nearest', cmap='Blues')\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a regressão já feita, podemos testá-la e obter sua matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic(X, Y, T, classes):\n",
    "    '''\n",
    "        Tests logistic regression with implemented metrics.\n",
    "        Accuracy, normalized accuracy, precision, recall, f1-score.\n",
    "    '''\n",
    "    \n",
    "    pred = predict(X, T)\n",
    "    met, conf = get_metrics(Y, pred, len(classes)) \n",
    "    plot_confusion_matrix(conf, classes, model = 'Multinomial Logistic Regression')\n",
    "    \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(f'Accuracy: {met[\"accuracy\"]:.4f}')\n",
    "    print(f'Normalized Accuracy: {met[\"norm_acc\"].mean():.4f}')\n",
    "    print(f'Precision per class: {met[\"precision\"]} (avg. precision: {met[\"precision\"].mean():.4f})')\n",
    "    print(f'Recall per class: {met[\"recall\"]} (avg. recall: {met[\"recall\"].mean():.4f})')\n",
    "    print(f'F1-Score per class: {met[\"f1\"]} (avg. f1-score: {met[\"f1\"].mean():.4f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "print(\"Training Metrics:\")\n",
    "test_logistic(Xb, Y, T, classes)\n",
    "    \n",
    "print()\n",
    "print(\"Validation Metrics:\")\n",
    "test_logistic(X_vb, Y_v, T, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Xb, X_vb, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o resultado está longe de satisfatório, devemos perseguir outras técnicas para resolver o problema.\n",
    "\n",
    "Podemos construir uma **rede neural densa** em seguida, para comparar o desempenho. A rede neural implementada possui três classes:\n",
    "- Meta, classe que possui informações do modelo, podendo ser utilizada para obter informações da execução também.\n",
    "- Network, classe que representa a rede, podendo ser construída com qualquer quantidade de camadas (maior que 2), com ou sem regularização, e podendo opcionalmente utilizar a função _softmax_ na última camada.\n",
    "- Optimizer, classe que implementa dois otimizadores para a rede neural: **Adadelta** e **Adam**, que podem ser opcionalmente utilizados.\n",
    "\n",
    "No momento, exploraremos a rede neural sem otimizadores. A rede implementa _batch_, _stochastic_ e _mini-batch_ _gradient descent_ . Há suporte também para salvar a rede em um arquivo via _NumPy_ ou carregar um arquivo que contém uma rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed based on current time\n",
    "seed(datetime.now())\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''Function for calculating the sigmoid and preventing overflow\n",
    "\n",
    "        Parameters:\n",
    "            x (float): Value for which the sigmoid will be calculated\n",
    "\n",
    "        Returns:\n",
    "            float : Sigmoid of x\n",
    "    '''\n",
    "    # The masks rounds values preventing np.exp to overflow\n",
    "    x[x >  50] =  50.0\n",
    "    x[x < -50] = -50.0\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta:\n",
    "    def __init__(self, T, m, batch_size, sampling=0):\n",
    "        self.index = 0  # Saves the position in the random list\n",
    "        self.iters = 0  # Counts the number of weights updates \n",
    "        self.bound = m  # Total amount of samples\n",
    "        self.epochs_count = 0      # Amount of completed epochs\n",
    "        self.start_time = time()   # Training start time\n",
    "        self.sampling = sampling   # Number of iterations which samples are collected\n",
    "        self.samples_list = list(range(m)) # Radomized samples for stochastic methods\n",
    "        self.history = {'loss':[], 'v_loss':[]}\n",
    "        shuffle(self.samples_list)\n",
    "        \n",
    "        # Checks it the batch is an integer (n samples) or percentage and, if \n",
    "        # it's a percentage, adjust to number of samples\n",
    "        if not isinstance(batch_size, int) :\n",
    "            self.batch_size = int(np.ceil(m*batch_size))\n",
    "        else : \n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        # If analysis data will be kept, saves time and thetas\n",
    "        if (self.sampling):\n",
    "            self.coef = [deepcopy(T)]     # Keeps trained coeficients per epoch\n",
    "            self.time = [0.0]   # Marks when a ecpoch was complete\n",
    "\n",
    "    def update(self, T):\n",
    "        '''Updates hyperparameters (epoch count, samples ramdomization)\n",
    "        \n",
    "            Parameters:\n",
    "                T (list of np.ndarray) : Coeficients from the current epoch\n",
    "            Returns:\n",
    "                change (bool) : if true, epoch finished\n",
    "        '''\n",
    "        \n",
    "        # Update index (add samples used in iteration)\n",
    "        self.iters += 1\n",
    "        self.index += self.batch_size\n",
    "        change = False\n",
    "        \n",
    "        # If epoch completed\n",
    "        if self.index >= self.bound :\n",
    "            self.index = 0             # Reset samples index\n",
    "            self.epochs_count += 1     # Count finished epoch\n",
    "            shuffle(self.samples_list) # Reshuffle samples\n",
    "            change = True              # Returns finished epoch\n",
    "\n",
    "        # Data for further analysis (Consumes time and memory)\n",
    "        if (self.sampling and self.iters//self.sampling) :\n",
    "            self.iters = 0\n",
    "            self.time.append(time() - self.start_time) # Adds time until epoch is done\n",
    "            self.coef.append(deepcopy(T)) # Adds current epoch cost\n",
    "            \n",
    "        return change\n",
    "\n",
    "    def update_history(self, loss, v_loss):\n",
    "        self.history['loss'].append(loss)\n",
    "        self.history['v_loss'].append(v_loss)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        '''Get samples indexes for the next batch'''\n",
    "        return self.samples_list[self.index : self.index+self.batch_size]\n",
    "\n",
    "    def __str__(self):\n",
    "        out = f'<Meta Object at {hex(id(self))}>'\n",
    "        out += f'Samples per Epoch: {self.bound}'\n",
    "        out += f'Current samples index: {self.index}'\n",
    "        out += f'Batch size: {self.batch_size}'\n",
    "        out += f'Epochs complete so far: {self.epochs_count}'\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, model, f='sg', reg_lambda=0, T=0, seed=0):\n",
    "        '''Initializes coeficients tables with the network weights\n",
    "        \n",
    "        Parameters: \n",
    "            model (list) : List with the amount of nodes per layer (without bias)\n",
    "            f (String) : Identification for the function to be minimized\n",
    "            reg_lambda (float) : Regularization parameter for the network (0 disables regularization)\n",
    "            T (list of numpy.ndarray): If instanciated, uses T as initial thetas\n",
    "        '''\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.f = f\n",
    "        self.theta = []\n",
    "\n",
    "        # Generates a random seed based on current time\n",
    "        if not seed : int(divmod(time(), 1)[1])\n",
    "\n",
    "        # If no preset, instanciate thetas and set random initial thetas\n",
    "        for (n,m) in zip(model[1:],model[:-1]): \n",
    "            # Instanciate weights with Xavier initialization\n",
    "            rand = np.random.RandomState(seed=seed)\n",
    "            self.theta.append((np.sqrt(2/(m+1)) * rand.randn(n,m+1)).astype(np.float32))\n",
    "\n",
    "    def cost(self, X, Y):\n",
    "        '''Calculates the current cost for the given samples (features and outputs)\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): NxM matrix with N features and M samples\n",
    "            Y (numpy.ndarray): KxM matrix with K output nodes and M samples\n",
    "\n",
    "        Returns:\n",
    "            float : Total cost for the current network and the given samples \n",
    "        '''\n",
    "        reg = 0 # Regularization value (weight reduction)\n",
    "        e = 10**-6 # Offset used to avoid log(0) (prevents NaNs)\n",
    "        m = Y.shape[1]  # Get amount of samples\n",
    "        fun = lambda x : (x[:,1:]*x[:,1:]).sum() # Sum squared parameters without bias \n",
    "        H = self.frag_forward(X, 10) # Get output layer activation values\n",
    "\n",
    "        # Calculate cost function\n",
    "        if self.f == 'sg': # Use sigmoid cost\n",
    "            cost = -(Y*np.log(H+e) + (1-Y)*np.log((1+e)-H)).sum()/m\n",
    "        elif self.f == 'sm': # Use softmax cost\n",
    "            cost_mat = softmax(H)\n",
    "            cost = (-Y * np.log(cost_mat+e)).sum(axis=0).mean()\n",
    "\n",
    "        # Calculate regularization, if parameter is set\n",
    "        if self.reg_lambda : reg = self.reg_lambda*(sum(map(fun, self.theta))/(2*m))\n",
    "\n",
    "        return cost + reg\n",
    "\n",
    "    def cost_deriv(self, H, Y):\n",
    "        '''Calculates the current cost for the given samples (features and outputs)\n",
    "\n",
    "        Parameters:\n",
    "            H (numpy.ndarray): NxM matrix with output layer activation values (N node X M samples)\n",
    "            Y (numpy.ndarray): KxM matrix with K output nodes and M samples\n",
    "\n",
    "        Returns:\n",
    "            float : Total cost for the current network and the given samples \n",
    "        '''\n",
    "        m = Y.shape[1]  # Get amount of samples\n",
    "\n",
    "        if self.f =='sg': # Use sigmoid derivative\n",
    "            return H - Y\n",
    "        elif self.f == 'sm': # Use softmax derivative\n",
    "            return softmax(H) - Y\n",
    "\n",
    "    def forward(self, features, nodes=0):\n",
    "        '''Execute the forward propagation using the defined thetas\n",
    "        \n",
    "        Parameters: \n",
    "            features (numpy.ndarray) : Column vector with input features (without bias)\n",
    "            nodes (list) : if instanciated, saves the nodes activation values for every layer\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray : Array with the propagated value for the output layer\n",
    "        '''\n",
    "        m = features.shape[1] # Get amount of samples to be propagated\n",
    "\n",
    "        for table in self.theta :\n",
    "            features = np.insert(features, 0, 1, axis=0)\n",
    "            if isinstance(nodes, list) : nodes += [features] \n",
    "            features = sigmoid(table.dot(features))\n",
    "\n",
    "        if isinstance(nodes, list) : nodes += [features]\n",
    "        return features\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''Execute gradient calculation for the given thetas and samples\n",
    "        \n",
    "        Parameters: \n",
    "            X (numpy.ndarray) : NxM matrix with M samples and each sample with N features\n",
    "            Y (numpy.ndarray) : KxM matrix with M samples ana each samples with K output nodes\n",
    "\n",
    "        Returns:\n",
    "            list of numpy.ndarray : Gradients for each set of thetas in the network \n",
    "        '''\n",
    "        reg_lambda = self.reg_lambda # Regularization parameter\n",
    "        theta = self.theta # Alias for the parameters\n",
    "        m = Y.shape[1] # Amount of samples to be backpropagated\n",
    "\n",
    "        layer = [] # For keeping the activation values\n",
    "        grad = [np.zeros(i.shape) for i in theta] # For keeping the partial derivatives\n",
    "        H = self.forward(X, nodes=layer) # Calculate hypotesis for every output node of every sample\n",
    "        sigma = [np.zeros(i.shape) for i in layer[1:]] # For keeping the activation errors (except input layer)\n",
    "        reg = 0\n",
    "\n",
    "        sigma[-1] = self.cost_deriv(H, Y)\n",
    "        \n",
    "        # Back propagate error to hidden layers (does not propagate to bias nodes)\n",
    "        for i in reversed(range(1, len(sigma))):\n",
    "            sig_d = layer[i][1:,:]*(1-layer[i][1:,:]) # Remove bias from layers for backpropagation\n",
    "            sigma[i-1] = (theta[i][:,1:].T).dot(sigma[i])*sig_d # Remove bias from thetas as well\n",
    "\n",
    "        # Accumulate derivatives values for every theta (does not update thetas)\n",
    "        # - Biases are not regularized, so the bias weights are casted to zero\n",
    "        for i in range(len(grad)):\n",
    "            if reg_lambda : reg = np.insert(theta[i][:,1:]*reg_lambda, 0, 0, axis=1)\n",
    "            grad[i] = (grad[i] + sigma[i].dot(layer[i].T))/m + reg\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def train(self, X, Y, Xv, Yv, type='m', opt=None, t_lim=7000, e_lim=100000, rate=0.01, mb_size=32, sampling=0, betas=(0,0)):\n",
    "        '''Trains the model until one of the given limits are reached\n",
    "\n",
    "        Parameters:\n",
    "            X (Float 2dArray): The coeficient matrix.\n",
    "            Y (Float 2dArray): The results matrix.\n",
    "            type (int): The choice of descent ('s'-stoch|'m'-mini|'b'-batch)\n",
    "            opt (String): Selects the optimizer for the decent (None|adadelta|adam)\n",
    "\n",
    "        Returns:\n",
    "            Meta : Class containing the runtime info.\n",
    "        '''\n",
    "        # Initializes epochs metadata class\n",
    "        batch_size = {'b':Y.shape[1],'m':mb_size,'s':1}.get(type) # Get number of samples\n",
    "        data = Meta(self.theta, Y.shape[1], batch_size, sampling=sampling) # Saves hyperparameters and other info for analisys \n",
    "        optmizer = Optimizer(rate, choice=opt, T=self.theta, batch=mb_size, beta=betas)\n",
    "        best_loss = 0\n",
    "        best_T = self.theta.copy()\n",
    "\n",
    "        # Starting descent\n",
    "        while (time() - data.start_time) <= t_lim:\n",
    "            # Getting new Thetas\n",
    "            b = data.get_batch() # Get indexes for mini batch\n",
    "            grad = self.backprop(X[:,b], Y[:,b])    \n",
    "            delta = optmizer.optimize(grad)\n",
    "            \n",
    "            # Update coefficients\n",
    "            for i in range(len(delta)):\n",
    "                self.theta[i] += delta[i]\n",
    "\n",
    "            change = data.update(self.theta)\n",
    "            \n",
    "            # Epoch change\n",
    "            if change:\n",
    "                loss = self.cost(X,Y)\n",
    "                v_loss=self.cost(Xv,Yv)\n",
    "                data.update_history(loss, v_loss)\n",
    "                \n",
    "                # Updates best thetas\n",
    "                if best_loss > v_loss:\n",
    "                    best_loss = v_loss\n",
    "                    best_T = self.theta.copy()\n",
    "                print(f\"Epoch {data.epochs_count:04d}/{e_lim:04d}\", f\"loss: {loss:.4f} | val loss: {v_loss:.4f}\")\n",
    "            \n",
    "            # Check termination\n",
    "            if data.epochs_count >= e_lim : \n",
    "                print(\"NOTE: Epochs limit for descent reached.\")  \n",
    "                self.theta = best_T     \n",
    "                return data\n",
    "            \n",
    "        print(\"NOTE: Time limit for descent exceded.\")\n",
    "        self.theta = best_T\n",
    "        return data\n",
    "\n",
    "    def frag_forward(self, X, parts):\n",
    "        '''Wrapper for the forward propagation which splits the M samples in slices\n",
    "           Prevents numpy memory spikes during large matrix multiplications           \n",
    "\n",
    "        Parameters:\n",
    "            X (Float 2dArray): NxM matrix with N input feratures and M samples\n",
    "            type (int): The choice of descent ('s'-stoch|'m'-mini|'b'-batch).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray : Array with the propagated value for the output layer\n",
    "        '''\n",
    "        m = X.shape[1]\n",
    "        size = int(np.ceil(m/parts)) # Get size of each batch\n",
    "        out_layer = np.zeros((self.theta[-1].shape[0],m)) # Prealocate output layer\n",
    "        batches = [i*size for i in range(int(np.ceil(parts)+1))] # Slices indexes\n",
    "        for (s,e) in zip(batches[:-1], batches[1:]): # Propagate for each slice\n",
    "            out_layer[:,s:e] += self.forward(X[:,s:e])\n",
    "        return out_layer\n",
    "\n",
    "    def accuracy(self, X, Y, T=0):\n",
    "        '''Caculates the cost for a set of samples in the current network\n",
    "        \n",
    "            Parameters:\n",
    "                X (Float 2dArray): NxM matrix with N input features and M samples\n",
    "                Y (Int 2dArray): NxM matrix with the expected M output layers\n",
    "            \n",
    "            Returns:\n",
    "                float : Percentage of correct predictions for the M samples\n",
    "        '''\n",
    "        m = Y.shape[1] \n",
    "        H = self.frag_forward(X, 10)\n",
    "        H = H.argmax(axis=0)\n",
    "        Y = Y.argmax(axis=0)\n",
    "        hits = (H==Y).sum()\n",
    "        return hits*100/m\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "           Predicts classes of examples.\n",
    "           \n",
    "           Parameters:\n",
    "                X (Float 2dArray): NxM matrix with N input features and M samples\n",
    "                \n",
    "           Returns:\n",
    "                Int Array: N-dimensional array with predictions\n",
    "        '''\n",
    "        \n",
    "        H = self.frag_forward(X, 10)\n",
    "        H = H.argmax(axis=0)\n",
    "        return H\n",
    "\n",
    "    def save(self, file_name):\n",
    "        '''Function for saving the current network to a file'''\n",
    "        save_list = [np.array([self.reg_lambda,self.f])]\n",
    "        save_list += self.theta\n",
    "        np.savez(file_name, save_list)\n",
    "        print(f\"Model saved as {file_name}\")\n",
    "    \n",
    "    def load(self, file_name):\n",
    "        '''Function for loading a Network from a file'''\n",
    "        obj = np.load(file_name)\n",
    "        self.reg_lambda = int(obj['arr_0'][0][0])\n",
    "        self.f = str(obj['arr_0'][0][1])\n",
    "        self.theta = []\n",
    "        for T in obj['arr_0'][1:]:\n",
    "            self.theta.append(T)\n",
    "        print(f\"Model {file_name} loaded\")\n",
    "\n",
    "    def __str__(self):\n",
    "        funcs = {'sg':'Sigmoid', 'sm':'Softmax'}\n",
    "        out = f'<Network Object at {hex(id(self))}>\\n'\n",
    "        out += f'Composed of {len(self.theta)+1} layers:\\n'\n",
    "        for i,n in enumerate(self.theta):\n",
    "            out += f'   layer {i+1} - {n.shape[1]} nodes\\n'\n",
    "        out += f'   Out layer - {self.theta[-1].shape[0]} nodes\\n'\n",
    "        out += f'Cost function: {funcs[self.f]}\\n'\n",
    "        out += f'Regularization parameter: {self.reg_lambda}\\n'\n",
    "        out += f'Amount of weights: {sum([x.size for x in self.theta])}\\n'\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, rate, choice=None, T=0, batch=0, beta=(0,0)):\n",
    "        self.choice = choice\n",
    "    \n",
    "        if choice=='adadelta':\n",
    "            self.e = 10**-8\n",
    "            self.decay = 0.99\n",
    "            self.batch = batch \n",
    "            self.avg = [np.zeros(t.shape) for t in T]\n",
    "            self.delta = [np.zeros(t.shape) for t in T]\n",
    "        elif choice=='adam':\n",
    "            self.e = 1e-8\n",
    "            self.rate = rate\n",
    "            self.t = 0\n",
    "            self.beta = beta\n",
    "            self.mt = [np.zeros(t.shape) for t in T]\n",
    "            self.vt = [np.zeros(t.shape) for t in T]\n",
    "            self.batch = batch\n",
    "        else:\n",
    "            self.rate = rate\n",
    "        \n",
    "    def optimize(self, grad):\n",
    "        \n",
    "        if self.choice=='adadelta':\n",
    "            delta = self.adadelta(grad)\n",
    "        elif self.choice=='adam':\n",
    "            delta = self.adam(grad)\n",
    "        else: # Vanilla\n",
    "            delta = [-self.rate*g for g in grad]\n",
    "        return delta\n",
    "\n",
    "    def adadelta(self, grad):\n",
    "        eps = self.e\n",
    "        decay = self.decay\n",
    "        batch = self.batch\n",
    "        new_deltas = []\n",
    "        for i,(g,avg,delta) in enumerate(zip(grad, self.avg, self.delta)):\n",
    "            # Calculate new optimized delta\n",
    "            avg = decay*avg + (1-decay)*np.square(g)\n",
    "            new_deltas.append(-(np.sqrt(delta+eps)/np.sqrt(avg+eps))*g)\n",
    "            # Updates for next iteration\n",
    "            self.avg[i] = avg\n",
    "            self.delta[i] = decay*delta + (1-decay)*np.square(new_deltas[-1])\n",
    "\n",
    "        return new_deltas\n",
    "    \n",
    "    def adam(self, grad):\n",
    "        self.t+=1\n",
    "        mt = self.mt\n",
    "        vt = self.vt\n",
    "        new_deltas = []\n",
    "        \n",
    "        for i,g in enumerate(grad):\n",
    "        \n",
    "            # Calculating moving averages\n",
    "            mt[i] = self.beta[0]*mt[i] + (1-self.beta[0])*g\n",
    "            vt[i] = self.beta[1]*vt[i] + (1-self.beta[1])*g*g\n",
    "            \n",
    "            # Bias-corrected estimates for moment\n",
    "            mt_b = mt[i]/(1-(self.beta[0]**self.t))\n",
    "            vt_b = vt[i]/(1-(self.beta[1]**self.t))\n",
    "            \n",
    "            delta = -1 * (self.rate * mt_b)/(np.sqrt(vt_b)+self.e)\n",
    "            new_deltas.append(delta)\n",
    "        \n",
    "        # Update\n",
    "        self.mt = mt\n",
    "        self.vt = vt\n",
    "        \n",
    "        return new_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com algumas adaptações, as funções de teste podem ser as mesmas. Porém, a entrada necessita de pré-processamento, pois ao contrário da regressão logística, a rede neural implementada utiliza _One-Hot Encoding_ na camada de saída.\n",
    "\n",
    "O método _predict_ da classe _Network_ retorna a saída ao formato original, para avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_layers(array):\n",
    "    '''Converts values from a list to one hot enconded output layers\n",
    "\n",
    "        Parameters:\n",
    "            array (list of ints): Contains the integer ID for each class\n",
    "    \n",
    "        Returns:\n",
    "            np.ndarray : 2D array where each colum is a output layer\n",
    "    '''\n",
    "    lower = min(array)\n",
    "    upper = max(array)\n",
    "    lines = upper-lower+1\n",
    "    new_arr = np.zeros((lines,len(array)), dtype=np.int8)\n",
    "\n",
    "    for j,i in enumerate(map(lambda x : x-lower, array)):\n",
    "       new_arr[i,j] = 1\n",
    "\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer um _wrapper_ para criar a rede, com uma arquitetura testada anteriormente que forneceu bons resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(X, Y, Yv, reg=0, f='sg'):\n",
    "    '''\n",
    "        Creates fully-connected neural network.\n",
    "        Parameters:\n",
    "            X,Xv: MxN matrix with N input features and M samples\n",
    "            Y,Yv: M-dimensional array with target values.\n",
    "            reg : regularization parameter\n",
    "            f   : activation function of choice (sg: sigmoid | sm: softmax)\n",
    "    '''\n",
    "    \n",
    "    # Adjusting input matrices (assumes X is normalized)\n",
    "    Yn = out_layers(Y)\n",
    "    Yvn=out_layers(Yv)\n",
    "    print(\"Handled input\")\n",
    "    \n",
    "    # Builds network object\n",
    "    feat, out = X.T.shape[0], Yn.shape[0]\n",
    "    h1, h2 = 256, 128\n",
    "    arc = [feat, h1, h2, out]\n",
    "    model = Network(arc, f=f, seed=23, reg_lambda=reg)\n",
    "    print('Created model')\n",
    "    print(\"Initial Accuracy:\", model.accuracy(X.T, Yn))\n",
    "    \n",
    "    return Yn, Yvn, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, temos o que precisamos para treinar a rede, que utilizará **Mini-Batch Gradient Descent**.\n",
    "Após diversos testes, bons parâmetros encontrados consistem em utilizar batches de tamanho 256, e _learning rate_ de 0.01. A função _softmax_ não foi utilizada pois seu resultado é pior. Os outros parâmetros foram escolhidos arbitrariamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yn, Yvn, model = create_neural_network(X, Y, Y_v)\n",
    "data = model.train(X.T, Yn, X_v.T, Yvn, type='m', mb_size=256, e_lim=500, t_lim=200, rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os testes são feitos de maneira similar à regressão logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(X, Y, model, classes):\n",
    "    '''\n",
    "        Function to test neural network.\n",
    "    '''\n",
    "    pred = model.predict(X)\n",
    "    met, conf = get_metrics(Y, pred, len(classes)) \n",
    "    plot_confusion_matrix(conf, classes, model='Neural Network')\n",
    "    \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(f'Accuracy: {met[\"accuracy\"]:.4f}')\n",
    "    print(f'Normalized Accuracy: {met[\"norm_acc\"].mean():.4f}')\n",
    "    print(f'Precision per class: {met[\"precision\"]} (avg. precision: {met[\"precision\"].mean():.4f})')\n",
    "    print(f'Recall per class: {met[\"recall\"]} (avg. recall: {met[\"recall\"].mean():.4f})')\n",
    "    print(f'F1-Score per class: {met[\"f1\"]} (avg. f1-score: {met[\"f1\"].mean():.4f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Metrics:\")\n",
    "test_neural_network(X.T, Y, model, classes)\n",
    "    \n",
    "print(\"Validation Metrics:\")\n",
    "test_neural_network(X_v.T, Y_v, model, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se ver que o resultado é melhor do que o visto para a regressão logística, porém ainda pode melhorar.\n",
    "Podemos utilizar os otimizadores implementados, e comparar os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos testes com otimizadores, os hiperparâmetros mudaram. Um batch size de 1024, por exemplo, não funciona bem para a rede original, mas funciona melhor que o tamanho original para os otimizadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_cost(X, Y, Xv, Yv, m=80000):\n",
    "    plot_info = []\n",
    "\n",
    "    samples = sample(range(Y.shape[1]), m)\n",
    "    X = X[:,samples]\n",
    "    Y = Y[:,samples]\n",
    "    feat = X.shape[0]\n",
    "    out = Y.shape[0]\n",
    "\n",
    "    # Set constant hyperparameters\n",
    "    e=50\n",
    "    t=200\n",
    "    r=0.001\n",
    "    b=1024\n",
    "    \n",
    "    # Variable aspects/hyperparams per plot\n",
    "    opt = ['adam','adadelta', None]\n",
    "    title = ['Adam', 'Adadelta', 'Vanilla']\n",
    "    arc = [3072, 256, 128, 10]\n",
    "\n",
    "    print(\"Architecture:\", arc)\n",
    "    for i in range(len(title)):\n",
    "        model = Network(arc, seed=23)\n",
    "        data = model.train(X, Y, Xv, Yv,\n",
    "                            opt=opt[i], \n",
    "                            type='m', \n",
    "                            t_lim=t, \n",
    "                            e_lim=e, \n",
    "                            rate=r, \n",
    "                            mb_size=b, \n",
    "                            betas=(0.9,0.999))\n",
    "        plot_info.append((title[i], range(0,data.epochs_count), data.history['loss']))\n",
    "        plot_info.append((title[i]+'(Validation)', range(0,data.epochs_count), data.history['v_loss']))\n",
    "        print(title[i]+':', model.accuracy(Xv,Yv), '%')\n",
    "\n",
    "\n",
    "    for (l,x,y) in plot_info : plt.plot(x, y, label=l)\n",
    "    plt.title(f\"Learning Curve \\n {m} samples | {t} sec\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, data\n",
    "gc.collect()\n",
    "optimization_cost(X.T, Yn, X_v.T, Yvn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É perceptível no resultado das comparações que o modelo com otimizador Adam sofreu _overfit_ , e por isso o custo de validação volta a subir no final. Esse modelo também é o que gerou melhores resultados nesse teste, então será considerado o melhor.\n",
    "\n",
    "Para impedir _overfitting_ , no treinamento final será utilizado um fator de regularização de 0.002, assim deixando a convergência mais lenta, então o tempo limite foi aumentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yn, Yvn, model = create_neural_network(X, Y, Y_v, reg=0.002)\n",
    "data = model.train(X.T, Yn, X_v.T, Yvn, type='m', mb_size=1024, e_lim=500, t_lim=900, rate=0.001, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Metrics:\")\n",
    "test_neural_network(X.T, Y, model, classes)\n",
    "    \n",
    "print(\"Validation Metrics:\")\n",
    "test_neural_network(Xv.T, Yv, model, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esse modelo foi escolhido como o melhor, podemos verificar o **conjunto de teste** nele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, Y, Yn, Yvn, X_v, Y_v, train, valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('Dataset/test.npz')\n",
    "X_t, Y_t = test['xs'], test['ys'].astype('int8')\n",
    "X_t = normalize_data(X_t, stats, choice)\n",
    "print(\"Test Data Normalized!\")\n",
    "\n",
    "Ytn = out_layers(Y_t)\n",
    "print(\"Test Metrics:\")\n",
    "test_neural_network(Xt.T, Ytn, model, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
