{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2: Regressão Logística e Redes Neurais\n",
    "\n",
    "O projeto consiste em explorar técnicas de classificação utilizando o _dataset_ [CINIC-10](https://github.com/BayesWatch/cinic-10), que consiste em um conjunto de 100000 imagens anotadas em 10 classes, que são:\n",
    "\n",
    "- Airplane\n",
    "- Automobile\n",
    "- Bird\n",
    "- Cat\n",
    "- Deer\n",
    "- Dog\n",
    "- Frog\n",
    "- Horse\n",
    "- Ship\n",
    "- Truck\n",
    "\n",
    "Para resolver o problema, serão implementadas soluções que utilizam regressão logística multinomial ou rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "from random import seed, shuffle, sample\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _dataset_ a ser utilizado já está separado em conjuntos de treinamento, validação e teste, em arquivos **.npz**. Inicialmente, consideraremos apenas os conjuntos de treinamento e validação. Podemos economizar memória lendo os valores alvo como _int8_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('Dataset/train.npz')\n",
    "valid = np.load('Dataset/val.npz')\n",
    "X, X_v  = train['xs'], valid['xs']\n",
    "Y, Y_v = train['ys'].astype('int8') , valid['ys'].astype('int8')\n",
    "\n",
    "# Constantes de classe\n",
    "classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados em memória, podemos **normalizá-los** de diversas maneiras. As estatísticas utilizadas para normalização são as mesmas para todos os conjuntos. Esse processo pode consumir bastante memória então, para economia, sempre que possível os tipos serão alterados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(data, choice=1):\n",
    "    means,stds,mins,ranges = [],[],[],[]\n",
    "    \n",
    "    # Stats for normalization\n",
    "    if choice == 1 or choice == 3:\n",
    "        mins = np.apply_along_axis(np.amin, 0, data).astype('int16')\n",
    "        maxs = np.apply_along_axis(np.amax, 0, data).astype('int16')\n",
    "        ranges = maxs - mins\n",
    "    if choice == 2 or choice == 3:\n",
    "        means = np.apply_along_axis(np.mean, 0, data).astype('float16')\n",
    "    if choice == 2:\n",
    "        stds = np.apply_along_axis(np.std, 0, data).astype('float16')\n",
    "    \n",
    "    return {'mean':means, 'std':stds, 'mins':mins, 'range':ranges}\n",
    "    \n",
    "def normalize_data(data, stats, choice=1):\n",
    "    ''' Returns the normalized dataset.\n",
    "    \n",
    "        Parameters:\n",
    "            data (array) : numpy array with the dataset.\n",
    "            stats (array): numpy array with stats given by \"get_stats\". 0: means. 1: stds. 2:mins. 3:ranges\n",
    "            choice (int) : integer indicating the transformation to be used.\n",
    "\n",
    "        Returns:\n",
    "            data (array list):  transformed data (original data is lost).\n",
    "    '''\n",
    "\n",
    "    #### Transforming the dataset ####\n",
    "    # Min-max normalization\n",
    "    if choice == 1:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mins'])/stats['range'], 1, data).astype('float32')\n",
    "            \n",
    "    # Standardization\n",
    "    elif choice == 2:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mean'])/stats['std'], 1, data).astype('float32')\n",
    "            \n",
    "    # Mean normalization\n",
    "    elif choice == 3:\n",
    "        data = np.apply_along_axis(lambda x: (x - stats['mean'])/stats['range'], 1, data).astype('float32')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os dados normalizados, podemos começar a trabalhar com eles. Primeiramente, foi implementada uma **Regressão Logística Multinomial** para tentar solucionar o problema, utilizando a função _softmax_ para múltiplas classes. \n",
    "\n",
    "Foi utilizado **Batch Gradient Descent**, sem _early stopping_ e salvando o melhor conjunto de coeficientes.\n",
    "\n",
    "Para melhor visualização, os custos em cada conjunto por época foram guardados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def softmax(x):\n",
    "    x -= np.max(x, axis=1, keepdims=True)          # Numeric Stability\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def prob(X, T):\n",
    "    return softmax(X.dot(T))\n",
    "\n",
    "def predict(X, T):\n",
    "    y_scores = X.dot(T)\n",
    "    return np.argmax(y_scores, axis=1)\n",
    "\n",
    "# Cost function\n",
    "def cost(Y, Y_probs):\n",
    "    correct_probs = Y_probs[np.arange(Y.size), Y]\n",
    "    return (-1 * log(correct_probs)).mean()\n",
    "\n",
    "def cost_derivative(X, Y, Y_probs):\n",
    "    Y_probs[np.arange(Y.size),Y] -= 1\n",
    "    Y_probs /= Y.size\n",
    "    return X.T.dot(Y_probs)\n",
    "\n",
    "def log(x, bound=1e-16):\n",
    "    return np.log(np.maximum(x,bound))\n",
    "\n",
    "# Gradient Descent\n",
    "def gd_step(X, Y, T, Y_prob, alpha):\n",
    "    return T - alpha * cost_derivative(X, Y, Y_prob)\n",
    "\n",
    "def gradient_descent(X, Y, X_v, Y_v, T, alpha=0.01, e_lim=300):\n",
    "    \n",
    "    # First losses and scores\n",
    "    Y_prob = prob(X, T)\n",
    "    Y_v_prob = prob(X_v, T)\n",
    "    loss = cost(Y, Y_prob)\n",
    "    best_loss = cost(Y_v, Y_v_prob)\n",
    "\n",
    "    # History\n",
    "    best_T = T.copy()\n",
    "    history = {'cost': [loss], 'v_cost': [best_loss]}\n",
    "    \n",
    "    # Descent\n",
    "    for i in range(e_lim):\n",
    "        \n",
    "        # New theta\n",
    "        T = gd_step(X, Y, T, Y_prob, alpha)\n",
    "        \n",
    "        # New scores and losses\n",
    "        Y_prob = prob(X, T)\n",
    "        loss = cost(Y, Y_prob)\n",
    "        \n",
    "        # Validation\n",
    "        Y_v_prob = prob(X_v, T)\n",
    "        v_loss = cost(Y_v, Y_v_prob)\n",
    "        \n",
    "        # Updating best loss\n",
    "        if v_loss < best_loss:\n",
    "            best_loss = v_loss\n",
    "            best_T = T.copy()\n",
    "        \n",
    "        # History\n",
    "        history['cost'].append(loss)\n",
    "        history['v_cost'].append(v_loss)\n",
    "        print(f\"Epoch {i+1:04d}/{e_lim:04d}\", f\"loss: {loss:.4f} | val loss: {v_loss:.4f}\")\n",
    "    \n",
    "    print()\n",
    "    return best_T, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a regressão pronta, basta iniciar os coeficientes para o treinamento. Para isso, foi utilizada a _Xavier Initialization_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_coefs(features, dim2, rand_seed=None):\n",
    "    rand = np.random.RandomState(seed=rand_seed)\n",
    "    return np.sqrt(2/(features + 1)) * rand.randn(features, dim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(X, X_v, Y, Y_v, alpha, n_epochs, n_classes):\n",
    "    '''\n",
    "        Runs multinomial logistic regression.\n",
    "        Uses Xavier Random Initialization of coefficients.\n",
    "        Gradient descent: softmax function.\n",
    "    '''\n",
    "    \n",
    "    T = init_coefs(X.shape[1], n_classes, 57).astype('float32')\n",
    "\n",
    "    print(\"Regression:\")\n",
    "    T, hist = gradient_descent(X, Y, X_v, Y_v, T, alpha, n_epochs)\n",
    "    return T, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando o _bias_ , coluna adicional no início do conjunto de exemplos, para representar o termo _intercept_ , podemos treinar o modelo.\n",
    "Após muitos testes, parâmetros bons para a regressão definidos foram _learning rate_ de 0.01 e limite de 300 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = 2\n",
    "stats = get_stats(X, choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "X = normalize_data(X, stats, choice)\n",
    "print(\"Training Data Normalized!\")\n",
    "X_v = normalize_data(X_v, stats, choice)\n",
    "print(\"Validation Data Normalized!\")\n",
    "\n",
    "Xb = np.insert(X, 0, 1, axis=1)\n",
    "X_vb = np.insert(X_v, 0, 1, axis=1)\n",
    "print(\"Bias Added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, history = run_logistic(Xb, X_vb, Y, Y_v, 0.01, 300, len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a regressão, pode-se verificar as curvas de aprendizado de treinamento e de validação por meio dos valores salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_with_history(history):\n",
    "    '''\n",
    "        Plots learning curves from history (dictionary of lists)\n",
    "    '''\n",
    "    keys = sorted(history.keys())\n",
    "    for k in keys:\n",
    "        plt.plot(history[k])\n",
    "        \n",
    "    plt.legend(keys, loc='upper left')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "learning_with_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar o desempenho do modelo, diversas métricas podem ser utilizadas, como:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "Porém, para múltiplas classes, apenas _accuracy_ possui definição escalável. As outras métricas estão definidas **por classe**, e a média pode ser utilizada como métrica única se desejado. Um modo fácil de calcular essas métricas é com a **matriz de confusão** da solução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the results.\n",
    "def confusion_matrix(Y, Y_pred, classes):\n",
    "    conf = np.zeros((classes,classes)).astype('int32')\n",
    "    for i in range(Y.size):\n",
    "        conf[Y[i], Y_pred[i]] += 1\n",
    "    return conf\n",
    "\n",
    "# Accuracy from confusion matrix. True/total\n",
    "def accuracy(confusion):\n",
    "    true_amount = confusion.trace()\n",
    "    total = confusion.sum()\n",
    "    return true_amount/total\n",
    "    \n",
    "# Precision per class from confusion matrix.\n",
    "def precision(confusion):\n",
    "    diag = np.diagonal(confusion)\n",
    "    return diag/confusion.sum(0)\n",
    "\n",
    "# Recall per class from confusion matrix.\n",
    "def recall(confusion):\n",
    "    diag = np.diagonal(confusion)\n",
    "    return diag/confusion.sum(1)\n",
    "\n",
    "# F1 Score per class from precision and recall\n",
    "def f1_score(prec, rec, bound=1):\n",
    "    return 2*prec*rec/(prec+rec+bound)\n",
    "    \n",
    "\n",
    "# Function to calculate metrics for evaluation\n",
    "def get_metrics(target, predictions, classes):\n",
    "    conf = confusion_matrix(target, predictions, classes)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy(conf)\n",
    "    prec= precision(conf)\n",
    "    rec = recall(conf)\n",
    "    f1 = f1_score(prec, rec)\n",
    "    avg_acc = (prec + rec)/2\n",
    "    \n",
    "    return {'accuracy':acc, 'norm_acc':avg_acc, 'precision':prec, 'recall':rec, 'f1':f1}, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz de confusão em si também é interessante de visualizar, por conter muitas informações pertinentes ao desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion, classes, model='Neural Network'):\n",
    "    '''\n",
    "        Plots an already created confusion matrix for a generic amount of classes.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    #Bounding box\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['right'].set_color('black')\n",
    "\n",
    "    plt.title('Confusion Matrix for ' + model)\n",
    "\n",
    "    #Ticks\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thr = confusion.max()/2\n",
    "    for i, j in it.product(range(confusion.shape[0]), range(confusion.shape[1])):\n",
    "        plt.text(j, i, confusion[i, j],\n",
    "            horizontalalignment='center',\n",
    "            color='white' if confusion[i, j] > thr else 'black')\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.imshow(confusion, interpolation='nearest', cmap='Blues')\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a regressão já feita, podemos testá-la e obter sua matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic(X, Y, T, classes):\n",
    "    '''\n",
    "        Tests logistic regression with implemented metrics.\n",
    "        Accuracy, normalized accuracy, precision, recall, f1-score.\n",
    "    '''\n",
    "    \n",
    "    pred = predict(X, T)\n",
    "    met, conf = get_metrics(Y, pred, len(classes)) \n",
    "    plot_confusion_matrix(conf, classes, model = 'Multinomial Logistic Regression')\n",
    "    \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(f'Accuracy: {met[\"accuracy\"]:.4f}')\n",
    "    print(f'Normalized Accuracy: {met[\"norm_acc\"].mean():.4f}')\n",
    "    print(f'Precision per class: {met[\"precision\"]} (avg. precision: {met[\"precision\"].mean():.4f})')\n",
    "    print(f'Recall per class: {met[\"recall\"]} (avg. recall: {met[\"recall\"].mean():.4f})')\n",
    "    print(f'F1-Score per class: {met[\"f1\"]} (avg. f1-score: {met[\"f1\"].mean():.4f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "print(\"Training Metrics:\")\n",
    "test_logistic(Xb, Y, T, classes)\n",
    "    \n",
    "print()\n",
    "print(\"Validation Metrics:\")\n",
    "test_logistic(X_vb, Y_v, T, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Xb, X_vb, T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o resultado está longe de satisfatório, devemos perseguir outras técnicas para resolver o problema.\n",
    "\n",
    "Podemos construir uma **rede neural densa** em seguida, para comparar o desempenho. A rede neural implementada possui três classes:\n",
    "- Meta, classe que possui metadados do treinamento do modelo, podendo ser utilizada para obter informações para análise.\n",
    "- Network, classe que representa a rede, podendo ser construída com qualquer quantidade de camadas (maior que 2), com ou sem regularização, e podendo opcionalmente utilizar a função _softmax_ na última camada.\n",
    "- Optimizer, classe que implementa dois otimizadores para a rede neural: **Adadelta** e **Adam**, que podem ser opcionalmente utilizados.\n",
    "\n",
    "No momento, exploraremos a rede neural sem otimizadores. A rede implementa _batch_, _stochastic_ e _mini-batch_ _gradient descent_ . Há suporte também para salvar a rede em um arquivo via _NumPy_ ou carregar um arquivo que contém uma rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed based on current time\n",
    "seed(datetime.now())\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''Function for calculating the sigmoid and preventing overflow\n",
    "\n",
    "        Parameters:\n",
    "            x (float): Value for which the sigmoid will be calculated\n",
    "\n",
    "        Returns:\n",
    "            float : Sigmoid of x\n",
    "    '''\n",
    "    # The masks rounds values preventing np.exp to overflow\n",
    "    x[x >  50] =  50.0\n",
    "    x[x < -50] = -50.0\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta:\n",
    "    def __init__(self, T, m, l, vl, batch_size, sampling=0):\n",
    "        self.index = 0  # Saves the position in the random list\n",
    "        self.iters = 0  # Counts the number of weights updates \n",
    "        self.bound = m  # Total amount of samples\n",
    "        self.best_loss = 0\n",
    "        self.best_T = T.copy()\n",
    "        self.epochs_count = 0      # Amount of completed epochs\n",
    "        self.start_time = time()   # Training start time\n",
    "        self.sampling = sampling   # Number of iterations which samples are collected\n",
    "        self.samples_list = list(range(m)) # Radomized samples for stochastic methods\n",
    "        self.history = {'loss':[l], 'v_loss':[vl]}\n",
    "        shuffle(self.samples_list)\n",
    "        \n",
    "        # Checks it the batch is an integer (n samples) or percentage and, if \n",
    "        # it's a percentage, adjust to number of samples\n",
    "        if not isinstance(batch_size, int) :\n",
    "            self.batch_size = int(np.ceil(m*batch_size))\n",
    "        else : \n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        # If analysis data will be kept, saves time and thetas\n",
    "        if (self.sampling):\n",
    "            self.coef = [deepcopy(T)] # Keeps trained coeficients per epoch\n",
    "            self.time = [0.0]   # Marks when a ecpoch was complete\n",
    "\n",
    "    def update(self, cost, samples_sets, T, e_lim):\n",
    "        '''Updates hyperparameters (epoch count, samples ramdomization)\n",
    "        \n",
    "            Parameters:\n",
    "                T (list of np.ndarray) : Coeficients from the current epoch\n",
    "                analysis (bool) : If true, keeps arguments for analysis\n",
    "            Returns:\n",
    "                change (bool) : if true, epoch finished\n",
    "        '''\n",
    "        \n",
    "        # Update index (add samples used in iteration)\n",
    "        self.iters += 1\n",
    "        self.index += self.batch_size\n",
    "        change = False\n",
    "        \n",
    "        # If epoch completed\n",
    "        if self.index >= self.bound :\n",
    "            self.index = 0             # Reset samples index\n",
    "            self.epochs_count += 1     # Count finished epoch\n",
    "            shuffle(self.samples_list) # Reshuffle samples\n",
    "            (X,Y,Xv,Yv) = samples_sets\n",
    "            loss = cost(X,Y)\n",
    "            v_loss = cost(Xv,Yv)\n",
    "            self.update_history(loss, v_loss)\n",
    "            \n",
    "            # Updates best thetas\n",
    "            if self.best_loss > v_loss:\n",
    "                self.best_loss = v_loss\n",
    "                self.best_T = self.theta.copy()\n",
    "            print(f\"Epoch {self.epochs_count:04d}/{e_lim:04d}\", f\"loss: {loss:.4f} | val loss: {v_loss:.4f}\")\n",
    "\n",
    "        # Data for further analysis (Consumes time and memory)\n",
    "        if (self.sampling and self.iters//self.sampling) :\n",
    "            self.iters = 0\n",
    "            self.time.append(time() - self.start_time) # Adds time until epoch is done\n",
    "            self.coef.append(deepcopy(T)) # Adds current epoch cost\n",
    "            \n",
    "        return change\n",
    "\n",
    "    def update_history(self, loss, v_loss):\n",
    "        self.history['loss'].append(loss)\n",
    "        self.history['v_loss'].append(v_loss)\n",
    "        \n",
    "    def get_batch(self):\n",
    "        '''Get samples indexes for the next batch'''\n",
    "        return self.samples_list[self.index : self.index+self.batch_size]\n",
    "\n",
    "    def __str__(self):\n",
    "        out = f'<Meta Object at {hex(id(self))}>'\n",
    "        out += f'Samples per Epoch: {self.bound}'\n",
    "        out += f'Current samples index: {self.index}'\n",
    "        out += f'Batch size: {self.batch_size}'\n",
    "        out += f'Epochs complete so far: {self.epochs_count}'\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, model, f='sg', reg_lambda=0, T=0, seed=0):\n",
    "        '''Initializes coeficients tables with the network weights\n",
    "        \n",
    "        Parameters: \n",
    "            model (list) : List with the amount of nodes per layer (without bias)\n",
    "            f (String) : Identification for the function to be minimized\n",
    "            reg_lambda (float) : Regularization parameter for the network (0 disables regularization)\n",
    "            T (list of numpy.ndarray): If instanciated, uses T as initial thetas\n",
    "        '''\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.f = f\n",
    "        self.theta = []\n",
    "\n",
    "        # Generates a random seed based on current time\n",
    "        if not seed : int(divmod(time(), 1)[1])\n",
    "\n",
    "        # If no preset, instanciate thetas and set random initial thetas\n",
    "        for (n,m) in zip(model[1:],model[:-1]): \n",
    "            # Instanciate weights with Xavier initialization\n",
    "            rand = np.random.RandomState(seed=seed)\n",
    "            self.theta.append((np.sqrt(2/(m+1)) * rand.randn(n,m+1)).astype(np.float32))\n",
    "\n",
    "    def cost(self, X, Y):\n",
    "        '''Calculates the current cost for the given samples (features and outputs)\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): NxM matrix with N features and M samples\n",
    "            Y (numpy.ndarray): KxM matrix with K output nodes and M samples\n",
    "\n",
    "        Returns:\n",
    "            float : Total cost for the current network and the given samples \n",
    "        '''\n",
    "        reg = 0 # Regularization value (weight reduction)\n",
    "        e = 10**-6 # Offset used to avoid log(0) (prevents NaNs)\n",
    "        m = Y.shape[1]  # Get amount of samples\n",
    "        fun = lambda x : (x[:,1:]*x[:,1:]).sum() # Sum squared parameters without bias \n",
    "        H = self.frag_forward(X, 10) # Get output layer activation values\n",
    "\n",
    "        # Calculate cost function\n",
    "        if self.f == 'sg': # Use sigmoid cost\n",
    "            cost = -(Y*np.log(H+e) + (1-Y)*np.log((1+e)-H)).sum()/m\n",
    "        elif self.f == 'sm': # Use softmax cost\n",
    "            cost_mat = softmax(H)\n",
    "            cost = (-Y * np.log(cost_mat+e)).sum(axis=0).mean()\n",
    "\n",
    "        # Calculate regularization, if parameter is set\n",
    "        if self.reg_lambda : reg = self.reg_lambda*(sum(map(fun, self.theta))/(2*m))\n",
    "\n",
    "        return cost + reg\n",
    "\n",
    "    def cost_deriv(self, H, Y):\n",
    "        '''Calculates the current cost for the given samples (features and outputs)\n",
    "\n",
    "        Parameters:\n",
    "            H (numpy.ndarray): NxM matrix with output layer activation values (N node X M samples)\n",
    "            Y (numpy.ndarray): KxM matrix with K output nodes and M samples\n",
    "\n",
    "        Returns:\n",
    "            float : Total cost for the current network and the given samples \n",
    "        '''\n",
    "        m = Y.shape[1]  # Get amount of samples\n",
    "\n",
    "        if self.f =='sg': # Use sigmoid derivative\n",
    "            return H - Y\n",
    "        elif self.f == 'sm': # Use softmax derivative\n",
    "            return softmax(H) - Y\n",
    "\n",
    "    def forward(self, features, nodes=0):\n",
    "        '''Execute the forward propagation using the defined thetas\n",
    "        \n",
    "        Parameters: \n",
    "            features (numpy.ndarray) : Column vector with input features (without bias)\n",
    "            nodes (list) : if instanciated, saves the nodes activation values for every layer\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray : Array with the propagated value for the output layer\n",
    "        '''\n",
    "        m = features.shape[1] # Get amount of samples to be propagated\n",
    "\n",
    "        for table in self.theta :\n",
    "            features = np.insert(features, 0, 1, axis=0)\n",
    "            if isinstance(nodes, list) : nodes += [features] \n",
    "            features = sigmoid(table.dot(features))\n",
    "\n",
    "        if isinstance(nodes, list) : nodes += [features]\n",
    "        return features\n",
    "\n",
    "    def backprop(self, X, Y):\n",
    "        '''Execute gradient calculation for the given thetas and samples\n",
    "        \n",
    "        Parameters: \n",
    "            X (numpy.ndarray) : NxM matrix with M samples and each sample with N features\n",
    "            Y (numpy.ndarray) : KxM matrix with M samples ana each samples with K output nodes\n",
    "\n",
    "        Returns:\n",
    "            list of numpy.ndarray : Gradients for each set of thetas in the network \n",
    "        '''\n",
    "        reg_lambda = self.reg_lambda # Regularization parameter\n",
    "        theta = self.theta # Alias for the parameters\n",
    "        m = Y.shape[1] # Amount of samples to be backpropagated\n",
    "\n",
    "        layer = [] # For keeping the activation values\n",
    "        grad = [np.zeros(i.shape) for i in theta] # For keeping the partial derivatives\n",
    "        H = self.forward(X, nodes=layer) # Calculate hypotesis for every output node of every sample\n",
    "        sigma = [np.zeros(i.shape) for i in layer[1:]] # For keeping the activation errors (except input layer)\n",
    "        reg = 0\n",
    "\n",
    "        sigma[-1] = self.cost_deriv(H, Y)\n",
    "        \n",
    "        # Back propagate error to hidden layers (does not propagate to bias nodes)\n",
    "        for i in reversed(range(1, len(sigma))):\n",
    "            sig_d = layer[i][1:,:]*(1-layer[i][1:,:]) # Remove bias from layers for backpropagation\n",
    "            sigma[i-1] = (theta[i][:,1:].T).dot(sigma[i])*sig_d # Remove bias from thetas as well\n",
    "\n",
    "        # Accumulate derivatives values for every theta (does not update thetas)\n",
    "        # - Biases are not regularized, so the bias weights are casted to zero\n",
    "        for i in range(len(grad)):\n",
    "            if reg_lambda : reg = np.insert(theta[i][:,1:]*reg_lambda, 0, 0, axis=1)\n",
    "            grad[i] = (grad[i] + sigma[i].dot(layer[i].T))/m + reg\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def train(self, X, Y, Xv, Yv, type='m', opt=None, t_lim=7000, e_lim=100000, rate=0.01, mb_size=32, sampling=0, betas=(0,0)):\n",
    "        '''Trains the model until one of the given limits are reached\n",
    "\n",
    "        Parameters:\n",
    "            X (Float 2dArray): The coeficient matrix.\n",
    "            Y (Float 2dArray): The results matrix.\n",
    "            type (int): The choice of descent ('s'-stoch|'m'-mini|'b'-batch)\n",
    "            opt (String): Selects the optimizer for the decent (None|adadelta|adam)\n",
    "\n",
    "        Returns:\n",
    "            Meta : Class containing the runtime info.\n",
    "        '''\n",
    "        # Initializes epochs metadata class\n",
    "        batch_size = {'b':Y.shape[1],'m':mb_size,'s':1}.get(type) # Get number of samples\n",
    "        data = Meta(self.theta, Y.shape[1], self.cost(X,Y), self.cost(Xv,Yv), batch_size, sampling=sampling) # Saves hyperparameters and other info for analisys \n",
    "        optmizer = Optimizer(rate, choice=opt, T=self.theta, batch=mb_size, beta=betas)\n",
    "\n",
    "        # Starting descent\n",
    "        while (time() - data.start_time) <= t_lim:\n",
    "            # Getting new Thetas\n",
    "            b = data.get_batch() # Get indexes for mini batch\n",
    "            grad = self.backprop(X[:,b], Y[:,b])    \n",
    "            delta = optmizer.optimize(grad)\n",
    "            \n",
    "            # Update coefficients\n",
    "            for i in range(len(delta)):\n",
    "                self.theta[i] += delta[i]\n",
    "\n",
    "            data.update(self.cost, (X,Y,Xv,Yv), self.theta, e_lim)\n",
    "            \n",
    "            # Check termination\n",
    "            if data.epochs_count >= e_lim : \n",
    "                print(\"NOTE: Epochs limit for descent reached.\")  \n",
    "                self.theta = data.best_T     \n",
    "                return data\n",
    "            \n",
    "        print(\"NOTE: Time limit for descent exceded.\")\n",
    "        self.theta = data.best_T\n",
    "        return data\n",
    "\n",
    "    def frag_forward(self, X, parts):\n",
    "        '''Wrapper for the forward propagation which splits the M samples in slices\n",
    "           Prevents numpy memory spikes during large matrix multiplications           \n",
    "\n",
    "        Parameters:\n",
    "            X (Float 2dArray): NxM matrix with N input feratures and M samples\n",
    "            type (int): The choice of descent ('s'-stoch|'m'-mini|'b'-batch).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray : Array with the propagated value for the output layer\n",
    "        '''\n",
    "        m = X.shape[1]\n",
    "        size = int(np.ceil(m/parts)) # Get size of each batch\n",
    "        out_layer = np.zeros((self.theta[-1].shape[0],m)) # Prealocate output layer\n",
    "        batches = [i*size for i in range(int(np.ceil(parts)+1))] # Slices indexes\n",
    "        for (s,e) in zip(batches[:-1], batches[1:]): # Propagate for each slice\n",
    "            out_layer[:,s:e] += self.forward(X[:,s:e])\n",
    "        return out_layer\n",
    "\n",
    "    def accuracy(self, X, Y, T=0):\n",
    "        '''Caculates the cost for a set of samples in the current network\n",
    "        \n",
    "            Parameters:\n",
    "                X (Float 2dArray): NxM matrix with N input features and M samples\n",
    "                Y (Int 2dArray): NxM matrix with the expected M output layers\n",
    "            \n",
    "            Returns:\n",
    "                float : Percentage of correct predictions for the M samples\n",
    "        '''\n",
    "        m = Y.shape[1] \n",
    "        H = self.frag_forward(X, 10)\n",
    "        H = H.argmax(axis=0)\n",
    "        Y = Y.argmax(axis=0)\n",
    "        hits = (H==Y).sum()\n",
    "        return hits*100/m\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "           Predicts classes of examples.\n",
    "           \n",
    "           Parameters:\n",
    "                X (Float 2dArray): NxM matrix with N input features and M samples\n",
    "                \n",
    "           Returns:\n",
    "                Int Array: N-dimensional array with predictions\n",
    "        '''\n",
    "        \n",
    "        H = self.frag_forward(X, 10)\n",
    "        H = H.argmax(axis=0)\n",
    "        return H\n",
    "\n",
    "    def save(self, file_name):\n",
    "        '''Function for saving the current network to a file'''\n",
    "        save_list = [np.array([self.reg_lambda,self.f])]\n",
    "        save_list += self.theta\n",
    "        np.savez(file_name, save_list)\n",
    "        print(f\"Model saved as {file_name}\")\n",
    " \n",
    "    def load(self, file_name):\n",
    "        '''Function for loading a Network from a file'''\n",
    "        obj = np.load(file_name)\n",
    "        self.reg_lambda = int(obj['arr_0'][0][0])\n",
    "        self.f = str(obj['arr_0'][0][1])\n",
    "        self.theta = []\n",
    "        for T in obj['arr_0'][1:]:\n",
    "            self.theta.append(T)\n",
    "        print(f\"Model {file_name} loaded\")\n",
    "\n",
    "    def __str__(self):\n",
    "        funcs = {'sg':'Sigmoid', 'sm':'Softmax'}\n",
    "        out = f'<Network Object at {hex(id(self))}>\\n'\n",
    "        out += f'Composed of {len(self.theta)+1} layers:\\n'\n",
    "        for i,n in enumerate(self.theta):\n",
    "            out += f'   layer {i+1} - {n.shape[1]} nodes\\n'\n",
    "        out += f'   Out layer - {self.theta[-1].shape[0]} nodes\\n'\n",
    "        out += f'Cost function: {funcs[self.f]}\\n'\n",
    "        out += f'Regularization parameter: {self.reg_lambda}\\n'\n",
    "        out += f'Amount of weights: {sum([x.size for x in self.theta])}\\n'\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, rate, choice=None, T=0, batch=0, beta=(0,0)):\n",
    "        self.choice = choice\n",
    "    \n",
    "        if choice=='adadelta':\n",
    "            self.e = 10**-8\n",
    "            self.decay = 0.99\n",
    "            self.batch = batch \n",
    "            self.avg = [np.zeros(t.shape) for t in T]\n",
    "            self.delta = [np.zeros(t.shape) for t in T]\n",
    "        elif choice=='adam':\n",
    "            self.e = 1e-8\n",
    "            self.rate = rate\n",
    "            self.t = 0\n",
    "            self.beta = beta\n",
    "            self.mt = [np.zeros(t.shape) for t in T]\n",
    "            self.vt = [np.zeros(t.shape) for t in T]\n",
    "            self.batch = batch\n",
    "        else:\n",
    "            self.rate = rate\n",
    "        \n",
    "    def optimize(self, grad):\n",
    "        \n",
    "        if self.choice=='adadelta':\n",
    "            delta = self.adadelta(grad)\n",
    "        elif self.choice=='adam':\n",
    "            delta = self.adam(grad)\n",
    "        else: # Vanilla\n",
    "            delta = [-self.rate*g for g in grad]\n",
    "        return delta\n",
    "\n",
    "    def adadelta(self, grad):\n",
    "        eps = self.e\n",
    "        decay = self.decay\n",
    "        batch = self.batch\n",
    "        new_deltas = []\n",
    "        for i,(g,avg,delta) in enumerate(zip(grad, self.avg, self.delta)):\n",
    "            # Calculate new optimized delta\n",
    "            avg = decay*avg + (1-decay)*np.square(g)\n",
    "            new_deltas.append(-(np.sqrt(delta+eps)/np.sqrt(avg+eps))*g)\n",
    "            # Updates for next iteration\n",
    "            self.avg[i] = avg\n",
    "            self.delta[i] = decay*delta + (1-decay)*np.square(new_deltas[-1])\n",
    "\n",
    "        return new_deltas\n",
    "    \n",
    "    def adam(self, grad):\n",
    "        self.t+=1\n",
    "        mt = self.mt\n",
    "        vt = self.vt\n",
    "        new_deltas = []\n",
    "        \n",
    "        for i,g in enumerate(grad):\n",
    "        \n",
    "            # Calculating moving averages\n",
    "            mt[i] = self.beta[0]*mt[i] + (1-self.beta[0])*g\n",
    "            vt[i] = self.beta[1]*vt[i] + (1-self.beta[1])*g*g\n",
    "            \n",
    "            # Bias-corrected estimates for moment\n",
    "            mt_b = mt[i]/(1-(self.beta[0]**self.t))\n",
    "            vt_b = vt[i]/(1-(self.beta[1]**self.t))\n",
    "            \n",
    "            delta = -1 * (self.rate * mt_b)/(np.sqrt(vt_b)+self.e)\n",
    "            new_deltas.append(delta)\n",
    "        \n",
    "        # Update\n",
    "        self.mt = mt\n",
    "        self.vt = vt\n",
    "        \n",
    "        return new_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com algumas adaptações, as funções de teste podem ser as mesmas. Porém, a entrada necessita de pré-processamento, pois ao contrário da regressão logística, a rede neural implementada utiliza _One-Hot Encoding_ na camada de saída.\n",
    "\n",
    "O método _predict_ da classe _Network_ retorna a saída ao formato original, para avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_layers(array):\n",
    "    '''Converts values from a list to one hot enconded output layers\n",
    "\n",
    "        Parameters:\n",
    "            array (list of ints): Contains the integer ID for each class\n",
    "    \n",
    "        Returns:\n",
    "            np.ndarray : 2D array where each colum is a output layer\n",
    "    '''\n",
    "    lower = min(array)\n",
    "    upper = max(array)\n",
    "    lines = upper-lower+1\n",
    "    new_arr = np.zeros((lines,len(array)), dtype=np.int8)\n",
    "\n",
    "    for j,i in enumerate(map(lambda x : x-lower, array)):\n",
    "       new_arr[i,j] = 1\n",
    "\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer um _wrapper_ para criar a rede, com uma arquitetura testada anteriormente que forneceu bons resultados.\n",
    "O wrapper também transpõe os dados de entrada, já que este, diferentemente da regressão logística, considera as linhas sendo Features e as colunas como Samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(X, Y, Yv, reg=0, f='sg'):\n",
    "    '''\n",
    "        Creates fully-connected neural network.\n",
    "        Parameters:\n",
    "            X,Xv: MxN matrix with N input features and M samples\n",
    "            Y,Yv: M-dimensional array with target values.\n",
    "            reg : regularization parameter\n",
    "            f   : activation function of choice (sg: sigmoid | sm: softmax)\n",
    "    '''\n",
    "    \n",
    "    # Adjusting input matrices (assumes X is normalized)\n",
    "    Yn = out_layers(Y)\n",
    "    Yvn=out_layers(Yv)\n",
    "    print(\"Handled input\")\n",
    "    \n",
    "    # Builds network object\n",
    "    feat, out = X.T.shape[0], Yn.shape[0]\n",
    "    h1, h2 = 256, 128\n",
    "    arc = [feat, h1, h2, out]\n",
    "    model = Network(arc, f=f, seed=23, reg_lambda=reg)\n",
    "    print('Created model')\n",
    "    print(f\"Initial Accuracy: {(model.accuracy(X.T, Yn)/100):.4f}\")\n",
    "    \n",
    "    return Yn, Yvn, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, temos o que precisamos para treinar a rede, que utilizará **Mini-Batch Gradient Descent**.\n",
    "Após diversos testes, bons parâmetros encontrados consistem em utilizar batches de tamanho 256, e _learning rate_ de 0.01. A função _softmax_ não foi utilizada pois seu resultado é pior. Os outros parâmetros foram escolhidos arbitrariamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yn, Yvn, model = create_neural_network(X, Y, Y_v)\n",
    "data = model.train(X.T, Yn, X_v.T, Yvn, type='m', mb_size=256, e_lim=500, t_lim=300, rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os testes são feitos de maneira similar à regressão logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(X, Y, model, classes):\n",
    "    '''\n",
    "        Function to test neural network.\n",
    "    '''\n",
    "    pred = model.predict(X)\n",
    "    met, conf = get_metrics(Y, pred, len(classes)) \n",
    "    plot_confusion_matrix(conf, classes, model='Neural Network')\n",
    "    \n",
    "    np.set_printoptions(precision=4)\n",
    "    print(f'Accuracy: {met[\"accuracy\"]:.4f}')\n",
    "    print(f'Normalized Accuracy: {met[\"norm_acc\"].mean():.4f}')\n",
    "    print(f'Precision per class: {met[\"precision\"]} (avg. precision: {met[\"precision\"].mean():.4f})')\n",
    "    print(f'Recall per class: {met[\"recall\"]} (avg. recall: {met[\"recall\"].mean():.4f})')\n",
    "    print(f'F1-Score per class: {met[\"f1\"]} (avg. f1-score: {met[\"f1\"].mean():.4f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Metrics:\")\n",
    "test_neural_network(X.T, Y, model, classes)\n",
    "    \n",
    "print(\"Validation Metrics:\")\n",
    "test_neural_network(X_v.T, Y_v, model, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se ver que o resultado é melhor do que o visto para a regressão logística, porém ainda pode melhorar.\n",
    "Podemos utilizar os otimizadores implementados, e comparar os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos testes com otimizadores, os hiperparâmetros mudaram. Um batch size de 1024, por exemplo, não funciona bem para a rede original, mas funciona melhor que o tamanho original para os otimizadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_cost(X, Y, Xv, Yv, m=80000):\n",
    "    plot_info = []\n",
    "\n",
    "    samples = sample(range(Y.shape[1]), m)\n",
    "    X = X[:,samples]\n",
    "    Y = Y[:,samples]\n",
    "    feat = X.shape[0]\n",
    "    out = Y.shape[0]\n",
    "\n",
    "    # Set constant hyperparameters\n",
    "    e=50\n",
    "    t=200\n",
    "    r=0.001\n",
    "    b=1024\n",
    "    \n",
    "    # Variable aspects/hyperparams per plot\n",
    "    opt = ['adam','adadelta', None]\n",
    "    title = ['Adam', 'Adadelta', 'Vanilla']\n",
    "    arc = [3072, 256, 128, 10]\n",
    "\n",
    "    print(\"Architecture:\", arc)\n",
    "    for i in range(len(title)):\n",
    "        model = Network(arc, seed=23)\n",
    "        data = model.train(X, Y, Xv, Yv,\n",
    "                            opt=opt[i], \n",
    "                            type='m', \n",
    "                            t_lim=t, \n",
    "                            e_lim=e, \n",
    "                            rate=r, \n",
    "                            mb_size=b, \n",
    "                            betas=(0.9,0.999))\n",
    "        plot_info.append((title[i], range(0,data.epochs_count+1), data.history['loss']))\n",
    "        plot_info.append((title[i]+'(Validation)', range(0,data.epochs_count+1), data.history['v_loss']))\n",
    "        print(title[i]+':', model.accuracy(Xv,Yv), '%')\n",
    "\n",
    "\n",
    "    for (l,x,y) in plot_info : plt.plot(x, y, label=l)\n",
    "    plt.title(f\"Learning Curve \\n {m} samples | {t} sec\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, data\n",
    "gc.collect()\n",
    "optimization_cost(X.T, Yn, X_v.T, Yvn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É perceptível no resultado das comparações que o modelo com otimizador Adam sofreu _overfit_ , e por isso o custo de validação volta a subir no final. Esse modelo também é o que gerou melhores resultados nesse teste, então será considerado o melhor.\n",
    "\n",
    "Para impedir _overfitting_ , no treinamento final será utilizado um fator de regularização de 0.002, assim deixando a convergência mais lenta, então o tempo limite foi aumentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yn, Yvn, model = create_neural_network(X, Y, Y_v, reg=0.002)\n",
    "data = model.train(X.T, Yn, X_v.T, Yvn, type='m', opt='adam', mb_size=1024, e_lim=500, t_lim=900, rate=0.001, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Metrics:\")\n",
    "test_neural_network(X.T, Y, model, classes)\n",
    "    \n",
    "print(\"Validation Metrics:\")\n",
    "test_neural_network(X_v.T, Y_v, model, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esse modelo foi escolhido como o melhor, podemos verificar o **conjunto de teste** nele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, Y, Yn, Yvn, X_v, Y_v, train, valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test = np.load('Dataset/test.npz')\n",
    "X_t, Y_t = test['xs'], test['ys'].astype('int8')\n",
    "X_t = normalize_data(X_t, stats, choice)\n",
    "print(\"Test Data Normalized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Metrics:\")\n",
    "test_neural_network(X_t.T, Y_t, model, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
